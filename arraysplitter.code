<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/README.md</path>
<content>
# ArraySplitter: De Novo Decomposition of Satellite DNA Arrays

Decomposes satellite DNA arrays into monomers within telomere-to-telomere (T2T) assemblies. Ideal for analyzing centromeric and pericentromeric regions on monomeric level.

**Status:** In development. Optimized for 100Kb scale arrays; longer arrays will work but may take longer to process. Signigicanlty longer time.

**Update:** From 1.1.6, ArraySplitter now successfully decomposes arrays on megabase scale. Largest arrays takes around 5 minutes to process. Fortunatelly, there are only 41 arrays large 1 Mb in CHM13v20 assembly. And I'm going to add parallel processing to speed up singificantly the process. Currently, it is single-threaded.

**Update:** Monomers are required some polising of borders, I am working on it.

**Update:** To test ArraySplitter, I used CHM13v20 assembly, it takes around 3 hours, to decompose all arrays longer than 1 Kb (13K arrays).

## Installation

**Prerequisites**

* Python 3.6 or later

**Installation with pip:**

```bash
pip install arraysplitter
```

## Usage

**Basic Example**

```bash
time arraysplitter -i chr1.arrays.fa -o chr1.arrays
```

It will create a FASTA file with monomers separated by spaces.

**Explanation**

* **`-i chr1.arrays.fa`:**  FASTA file of satDNA arrays.
* **`-o chr1.arrays`:** Prefix for the output FASTA containing decomposed monomers (separated by spaces).

**All Options** 

```bash
arraysplitter --help 
```

## Rotating monomers to start with the same sequence

We found that different arrays of the same repeat family can be decomposed sligtly differently. To make them comparable, ArraySplitter can rotate monomers to start with the same sequence. 

```bash
arraysplitter_rotate -i arrays.fa -o arrays.norm.fa
```

And you can give the sequence to start with:

```bash
arraysplitter_rotate -i arrays.fa -o arrays.norm.fa -s TTTC
```

**Explanation**

* **`-i arrays.fa`:**  FASTA file of monomers.
* **`-o arrays.norm.fa`:** Output FASTA file with rotated monomers.

## Extracting and counting monomers

And finally, you can extract and count monomers from the arrays:

```bash
arraysplitter_extract -i arrays.norm.fa -o arrays.norm
```

It will create a file with monomer length, monomer frequency, and monomer sequence (ordered by frequency). For example, for the arrays.norm.fa file above, the output will be like this:

```bash
514     10      ATCCCATTCC
514     10      GATTGGAGTG
514     6       TCCTTT
514     5       TGCTG
514     10      ATTGAATGGA
514     10      ATGCAATGGA
514     5       TCCTA
```


## Contact

For questions or support: ad3002@gmail.com

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/setup.py</path>
<content>
import re

from setuptools import find_packages, setup

with open("README.md", "r", encoding="utf-8") as fh:
    long_description = fh.read()

version = None
for line in open("./src/ArraySplitter/__init__.py"):
    m = re.search("__version__\s*=\s*(.*)", line)
    if m:
        version = m.group(1).strip()[1:-1]  # quotes
        break
assert version

setup(
    name="ArraySplitter",
    version=version,
    packages=find_packages('src'),
    package_dir={'': 'src'},
    package_data={"": ["README.md"]},
    python_requires=">=3.6",
    include_package_data=True,
    scripts=[],
    license="BSD",
    url="https://github.com/aglabx/ArraySplitter",
    author="Aleksey Komissarov",
    author_email="ad3002@gmail.com",
    description="De Novo Decomposition of Satellite DNA Arrays into Monomers within Telomere-to-Telomere Assemblies",
    long_description=long_description,
    long_description_content_type="text/markdown",
    install_requires=[
        "PyExp",
        "editdistance",
        "tqdm",
    ],
    classifiers=[
        "Development Status :: 4 - Beta",
        "Environment :: Console",
        "Intended Audience :: Science/Research",
        "License :: OSI Approved :: BSD License",
        "Operating System :: OS Independent",
        "Programming Language :: Python",
        "Topic :: Scientific/Engineering :: Bio-Informatics",
    ],
    entry_points={
        'console_scripts': [
            'arraysplitter = ArraySplitter.decompose:run_it',
            'arraysplitter_rotate = ArraySplitter.rotate:run_it',
            'arraysplitter_extract = ArraySplitter.extract:run_it',
        ],
    },
)

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/build/lib/ArraySplitter/decompose.py</path>
<content>



import argparse

import os
from collections import Counter
import re
from tqdm import tqdm

import editdistance as ed

from .core_functions.io.fasta_reader import \
    sc_iter_fasta_file
from .core_functions.io.satellome_reader import \
    sc_iter_satellome_file
from .core_functions.io.trf_reader import sc_iter_trf_file
from .core_functions.tools.fs_tree import \
    iter_fs_tree_from_sequence


def get_top1_nucleotide(array):
    ### Step 1. Find the most frequent nucleotide (TODO: check all nucleotides and find with the best final score
    c = Counter()
    for n in "ACTG":
        c[n] = array.count(n)
        # print(n, array.count(n))
    return c.most_common(1)[0][0]


def get_fs_tree(array, top1_nucleotide, cutoff):
    ### Step 2. Build fs_tree (TODO:  optimize it for long sequences)
    names_ = [i for i in range(len(array)) if array[i] == top1_nucleotide]
    positions_ = names_[::]
    # print(f"Starting positions: {len(positions_)}")
    return iter_fs_tree_from_sequence(
        array, top1_nucleotide, names_, positions_, cutoff
    )


def iterate_hints(array, fs_tree, depth):
    ### Step 3. Find a list of hints (hint is the sequenece for array cutoff)

    current_length = 0
    buffer = []
    for L, names, positions in fs_tree:
        if L != current_length:
            if buffer:
                max_n = 0
                found_seq = None
                for start, end, N in buffer:
                    if N > max_n:
                        max_n = N
                        found_seq = array[start : end + 1]
                yield current_length, found_seq, max_n
            buffer = []
            current_length = L
            if current_length > depth:
                break
        start = names[0]
        end = positions[0]
        N = len(names)
        buffer.append((start, end, N))
    if buffer:
        max_n = 0
        found_seq = None
        for start, end, N in buffer:
            if N > max_n:
                max_n = N
                found_seq = array[start : end + 1]
        yield current_length, found_seq, max_n


def compute_cuts(array, hints):
    ### Step 4. Find optimal cutoff
    best_cut_seq = None
    best_cut_score = 0.0
    best_period = None
    possible_solutions = Counter()
    solution2meta = {}
    for L, cut_sequence, N in hints:
        period2freq = Counter()
        for seq in array.split(cut_sequence):
            period2freq[len(seq + cut_sequence)] += 1
        period, period_freq = period2freq.most_common(1)[0]
        total_periods = sum(period2freq.values())
        score = period_freq / total_periods

        possible_solutions[period] += 1
        if not period in solution2meta:
            solution2meta[period] = [cut_sequence, score, period]
        else:
            if score > solution2meta[period][1]:
                solution2meta[period] = [cut_sequence, score, period]
        if score > best_cut_score:
            best_cut_score = score
            best_cut_seq = cut_sequence
            best_period = period
        # print(L, period, period_freq, score, possible_solutions, solution2meta)
    if not possible_solutions:
        return array, 0, len(array)
    best_period = possible_solutions.most_common(1)[0][0]
    best_cut_seq, best_cut_score, best_period = solution2meta[best_period]
    return best_cut_seq, best_cut_score, best_period


### Step 5a. Try to cut long monomers to expected
def refine_repeat_even(repeat, best_period):
    if len(repeat) % best_period == 0:
        start = 0
        for _ in range(len(repeat) // best_period):
            yield repeat[start : start + best_period]
            start += best_period
    else:
        yield repeat


def decompose_array_iter1(array, best_cut_seq, best_period, verbose=True):
    repeats2count = Counter()
    decomposition = []
    cuts = array.split(best_cut_seq)
    for ii, x in enumerate(cuts):
        if ii > 0:
            repeat = best_cut_seq + x
        elif not x:
            repeat = best_cut_seq
        else:
            repeat = x
        if not repeat:
            continue
        if len(repeat) == best_period:
            if verbose:
                print(len(repeat), repeat)
            repeats2count[repeat] += 1
            decomposition.append(repeat)
        else:
            # print(len(repeat), best_period)
            for repeat in refine_repeat_even(repeat, best_period):
                if verbose:
                    print(len(repeat), repeat)
                repeats2count[repeat] += 1
                decomposition.append(repeat)
    return decomposition, repeats2count


### Step 5b. Try to cut long monomers to expected
def refine_repeat_odd(repeat, best_period, most_common_monomer, verbose=False):
    if len(repeat) / best_period > 1.3:
        n = len(most_common_monomer)
        optimal_cut = 0
        best_ed = n

        begin_positions = [i for i in range(min(len(repeat) - n + 1, 5))]
        end_positions = [i for i in range(max(0, len(repeat) - n + 1 - 5), len(repeat) - n + 1)]

        for i in begin_positions+end_positions:
            rep_b = repeat[i : i + n]
            dist = ed.eval(most_common_monomer, rep_b)
            if dist < best_ed:
                best_ed = dist
                optimal_cut = i
                if verbose:
                    print(
                        "Optimal cut",
                        best_ed,
                        optimal_cut,
                        len(repeat[:optimal_cut]),
                        len(repeat[optimal_cut:]),
                    )
        if best_ed < n / 2:
            if optimal_cut == 0:
                optimal_cut += n
            a = repeat[:optimal_cut]
            b = repeat[optimal_cut:]
            if min(len(a), len(b)) < 0:  # n/3:
                yield repeat
            else:
                if a:
                    yield a
                if b:
                    yield b
        else:
            yield repeat
    else:
        yield repeat


def decompose_array_iter2(decomposition, best_period, repeats2count_ref, verbose=True):
    repeats2count = Counter()
    refined_decomposition = []
    most_common_monomer = None
    for monomer, tf in repeats2count_ref.most_common(1000):
        if len(monomer) == best_period:
            most_common_monomer = monomer
            break
    assert most_common_monomer
    for repeat in decomposition:
        if verbose:
            print("Repeat under consideration", len(repeat), repeat)
        for repeat in refine_repeat_odd(
            repeat, best_period, most_common_monomer, verbose=verbose
        ):
            if verbose:
                print("Added:", len(repeat), repeat)
            repeats2count[repeat] += 1
            refined_decomposition.append(repeat)
    return (
        refined_decomposition,
        repeats2count,
        len(refined_decomposition) != len(decomposition),
    )


def print_monomers(decomposition, repeats2count, best_period):
    start2tf = Counter()
    for monomer in decomposition:
        start2tf[monomer[:5]] += 1
    print(start2tf)

    most_common_monomer = None
    for monomer, tf in repeats2count.most_common(1000):
        if len(monomer) == best_period:
            most_common_monomer = monomer
            break
    assert most_common_monomer
    for repeat in decomposition:
        print(
            len(repeat),
            start2tf[repeat[:5]],
            repeat,
            ed.eval(repeat, most_common_monomer),
        )


def print_pause_clean(decomposition, repeats2count, best_period):
    print_monomers(decomposition, repeats2count, best_period)
    input("?")


#   clear_output(wait=True)


def decompose_array(array, depth=500, cutoff=20, verbose=False):
    ### Step 1. Find the most frequent nucleotide (TODO: check all nucleotides and find the one with the best final score)
    top1_nucleotide = get_top1_nucleotide(array)
    # print("top1_nucleotide:", top1_nucleotide)
    # top1_nucleotide = "A"
    ### Step 2. Build fs_tree (TODO: optimize it for long sequences)
    fs_tree = get_fs_tree(array, top1_nucleotide, cutoff=cutoff)
    ### Step 3. Find a list of hints (hint is the sequence for array cutting)
    ### Here I defined it as a sequence with maximal coverage in the original array for a given length
    hints = iterate_hints(array, fs_tree, depth)
    ### Step 4. Find the optimal cut sequence and the best period
    ### Defined as the maximal fraction of the cut sequence to the total cut sequence
    best_cut_seq, best_cut_score, best_period = compute_cuts(array, hints)

    ### Step 5. Cut the array
    ### The first iteration finds monomer frequencies
    # print("Firset iteration")
    decomposition, repeats2count = decompose_array_iter1(
        array, best_cut_seq, best_period, verbose=verbose
    )
    
    # assert "".join(decomposition) == array
    ### The second iteration tries to cut longer monomers to the expected length
    changed = True
    while changed:
        # print("Firset iteration", len(decomposition))
        decomposition, repeats2count, changed = decompose_array_iter2(
            decomposition, best_period, repeats2count, verbose=verbose
        )
        # assert "".join(decomposition) == array

    ### TODO: The third iteration tries to glue short dangling monomers to the nearest monomer

    return decomposition, repeats2count, best_cut_seq, best_cut_score, best_period


def get_array_generator(input_file, format):
    '''Get array generator by format.'''
    if format == "fasta":
        return sc_iter_fasta_file(input_file)
    if format == "trf":
        return sc_iter_trf_file(input_file)
    if format == "satellome":
        return sc_iter_satellome_file(input_file)
    
    print(f"Unknown format: {format}")
    exit(1)
    

def main(input_file, output_prefix, format, threads):
    """Main function."""

    sequences = get_array_generator(input_file, format)
    total = 0
    for _ in sequences:
        total += 1
    sequences = get_array_generator(input_file, format)

    print(f"Start processing")
    

    depth = 100
    cutoff = None
    verbose = False

    if output_prefix.endswith(".fasta"):
        print("Remove .fasta from output prefix")
        output_prefix = output_prefix[:-6]
    elif output_prefix.endswith(".fa"):
        print("Remove .fa from output prefix")
        output_prefix = output_prefix[:-3]

    output_file = f"{output_prefix}.decomposed.fasta"
    print(f"Output file: {output_file}")
    
    with open(output_file, "w") as fw:
        for header, array in tqdm(sequences, total=total):
            # print(len(array), end=" ")
            if not cutoff:
                if len(array) > 1_000_000:
                    cutoff = 1000
                elif len(array) > 100_000:
                    cutoff = 250
                elif len(array) > 10_000:
                    cutoff = 10
                else:
                    cutoff = 3
            (
                decomposition,
                repeats2count,
                best_cut_seq,
                best_cut_score,
                best_period,
            ) = decompose_array(array, depth=depth, cutoff=cutoff, verbose=verbose)

            # print("best period:", best_period, "len:", len(decomposition))
            # print_pause_clean(decomposition, repeats2count, best_period)

            fw.write(f">{header} {best_period}\n")
            fw.write(" ".join(decomposition) + "\n")


def run_it():
    parser = argparse.ArgumentParser(
        description="De novo decomposition of satellite DNA arrays into monomers"
    )
    parser.add_argument("-i", "--input", help="Input file", required=True)
    parser.add_argument(
        "--format",
        help="Input format: fasta, trf [fasta]",
        required=False,
        default="fasta",
    )
    parser.add_argument("-o", "--output", help="Output prefix", required=True)
    parser.add_argument(
        "-t", "--threads", help="Number of threads", required=False, default=4
    )
    args = parser.parse_args()

    input_file = args.input
    output_prefix = args.output
    format = args.format
    threads = int(args.threads)

    if not os.path.isfile(input_file):
        print(f"File {input_file} not found")
        exit(1)

    main(input_file, output_prefix, format, threads)

if __name__ == "__main__":
    run_it()
</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/build/lib/ArraySplitter/__init__.py</path>
<content>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# @created: 20.02.2024
# @author: Aleksey Komissarov
# @contact: ad3002@gmail.com

__version__ = "1.2.1"

# __all__ = []

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/build/lib/ArraySplitter/rotate.py</path>
<content>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# @created: 21.02.2024
# @author: Aleksey Komissarov
# @contact: ad3002@gmail.com

import argparse

from .core_functions.tools.rotation import rotate_arrays

from .core_functions.io.fasta_reader import sc_iter_fasta_file

def main(fasta_file, output_file, starting_kmer=None):

    arrays = []
    for ii, (header, seq) in enumerate(sc_iter_fasta_file(fasta_file)):
        arrays.append(seq)

    new_arrays = rotate_arrays(arrays, starting_kmer=starting_kmer)
    
    with open(output_file, "w") as fw:
        for ii, (header, seq) in enumerate(sc_iter_fasta_file(fasta_file)):
            fw.write(f'>{header}\n{" ".join(new_arrays[ii])}\n')
            
def run_it():
    parser = argparse.ArgumentParser(description='Rotate arrays to start from the same position.')
    parser.add_argument('-i', '--fasta', type=str, help='Fasta file', required=True)
    parser.add_argument('-o', '--output', type=str, help='Output file', required=True)
    parser.add_argument('-s', '--start', type=str, help='Starting kmer for rotation [None]', default=None)
    args = parser.parse_args()
    main(args.fasta, args.output, starting_kmer=args.start)

if __name__ == '__main__':
    run_it()
</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/build/lib/ArraySplitter/extract.py</path>
<content>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# @created: 21.02.2024
# @author: Aleksey Komissarov
# @contact: ad3002@gmail.com

from collections import Counter
import argparse
import logging
from .core_functions.io.fasta_reader import sc_iter_fasta_file

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def main(fasta_file, output_prefix, distance=0, function='hd'):

    if distance < 0:
        raise ValueError(f'Distance should be >= 0, but {distance} given')
    
    if function not in ['hd', 'ed']:
        raise ValueError(f'Function should be hd or ed, but {function} given')
    
    logging.info(f'Extract monomers from {fasta_file}')
    monomers = Counter()
    for header, seq in sc_iter_fasta_file(fasta_file):
        for monomer in seq.split():
            monomers[monomer] += 1

    logging.info(f'Found {len(monomers)} uniq monomers with total {sum(monomers.values())} monomers')

    logging.info(f'Filter monomers with distance <= {distance} using {function} function')
    # if function == 'hd':
    #     monomers = {k: v for k, v in monomers.items() if v <= distance}
    # else:
    #     raise NotImplementedError('Edit distance is not implemented yet')
    logging.info(f'Found {len(monomers)} monomers with distance <= {distance}')

    output_file = f'{output_prefix}.monomers.tsv'
    logging.info(f'Save monomers to {output_file}')
    with open(output_file, "w") as fw:
        for monomer, count in monomers.most_common(1000000000):
            fw.write(f'{count}\t{len(monomer)}\t{monomer}\n')
            
def run_it():
    parser = argparse.ArgumentParser(description='Extract monomers from array over edit distance.')
    parser.add_argument('-i', '--fasta', type=str, help='Fasta file with arrays', required=True)
    parser.add_argument('-o', '--output', type=str, help='Output prefix', required=True)
    parser.add_argument('-f', '--function', type=str, help='Distance function hamming (hd) or edit (ed) [hd]', default='hd')
    parser.add_argument('-d', '--distance', type=int, help='Distance value, you will get monomers over <= distance [0]', default=0)
    args = parser.parse_args()
    main(args.fasta, args.output, distance=args.distance, function=args.function)

if __name__ == '__main__':
    run_it()
</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/build/lib/ArraySplitter/core_functions/__init__.py</path>
<content>

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/build/lib/ArraySplitter/core_functions/tools/sequences.py</path>
<content>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# @created: 20.02.2024
# @author: Aleksey Komissarov
# @contact: ad3002@gmail.com


REVCOMP_DICTIONARY = dict(list(zip("ATCGNatcgn~[] ", "TAGCNtagcn~][ ")))

def get_revcomp(sequence):
    """Return reverse complementary sequence.

    >>> complementary('AT CG')
    'CGAT'

    """
    return "".join(
        REVCOMP_DICTIONARY.get(nucleotide, "") for nucleotide in reversed(sequence)
    )

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/build/lib/ArraySplitter/core_functions/tools/fs_tree.py</path>
<content>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# @created: 02.02.2024
# @author: Aleksey Komissarov
# @contact: ad3002@gmail.com

from collections import deque

import heapq

class WeightedValueHeap:
    def __init__(self):
        self.heap = []
    
    def insert(self, weight, value):
        heapq.heappush(self.heap, (weight, value))
    
    def pop(self):
        if self.heap:
            return heapq.heappop(self.heap)
        return None
    
    def peek(self):
        if self.heap:
            return self.heap[0]
        return None

def update(
    fs_tree,
    queue,
    nucl,
    fs_x,
    fs_xp,
    current_cid,
    cid,
    cutoff,
):
    if len(fs_x) > cutoff:
        fs_tree[cid] = (
            cid,
            [nucl],
            fs_x,
            fs_xp,
            None,
            [],
        )
        fs_tree[current_cid][5].append(cid)
        queue.append(cid)
        cid += 1
    return cid


def build_fs_tree_from_sequence(array, starting_seq_, names_, positions_, cutoff):
    fs_tree = {}
    queue = []
    cid = 0
    seq = starting_seq_
    names = names_[::]
    positions = positions_[::]
    children = []
    fs = (cid, [seq], names, positions, None, children)
    queue.append(cid)
    fs_tree[cid] = fs
    cid += 1
    cutoff = cutoff

    while queue:
        qcid = queue.pop()

        fs = fs_tree[qcid]

        fs_a, fs_c, fs_g, fs_t = [], [], [], []
        fs_ap, fs_cp, fs_gp, fs_tp = [], [], [], []

        for ii, pos in enumerate(fs[3]):
            current_cid = fs[0]
            seq = fs[1]
            name = fs[2][ii]

            if pos + 1 == len(array):
                continue

            nucl = array[pos + 1]
            if nucl == "A":
                fs_a.append(name)
                fs_ap.append(pos + 1)
            elif nucl == "C":
                fs_c.append(name)
                fs_cp.append(pos + 1)
            elif nucl == "G":
                fs_g.append(name)
                fs_gp.append(pos + 1)
            elif nucl == "T":
                fs_t.append(name)
                fs_tp.append(pos + 1)
            
        cid = update(
            fs_tree,
            queue,
            "A",
            fs_a,
            fs_ap,
            current_cid,
            cid,
            cutoff,
        )
        cid = update(
            fs_tree,
            queue,
            "C",
            fs_c,
            fs_cp,
            current_cid,
            cid,
            cutoff,
        )
        cid = update(
            fs_tree,
            queue,
            "G",
            fs_g,
            fs_gp,
            current_cid,
            cid,
            cutoff,
        )
        cid = update(
            fs_tree,
            queue,
            "T",
            fs_t,
            fs_tp,
            current_cid,
            cid,
            cutoff,
        )

    return fs_tree


def iter_fs_tree_from_sequence(array, starting_seq_, names_, positions_, cutoff):
    heap = []
    names = names_[::]
    positions = positions_[::]
    fs = (1, names, positions)
    heapq.heappush(heap, (1, fs))
    cutoff = cutoff
    while heap:
        L, fs = heapq.heappop(heap)

        fs_a, fs_c, fs_g, fs_t = [], [], [], []
        fs_ap, fs_cp, fs_gp, fs_tp = [], [], [], []

        for ii, pos in enumerate(fs[2]):
            
            name = fs[1][ii]

            if pos + 1 == len(array):
                continue

            nucl = array[pos + 1]
            if nucl == "A":
                fs_a.append(name)
                fs_ap.append(pos + 1)
            elif nucl == "C":
                fs_c.append(name)
                fs_cp.append(pos + 1)
            elif nucl == "G":
                fs_g.append(name)
                fs_gp.append(pos + 1)
            elif nucl == "T":
                fs_t.append(name)
                fs_tp.append(pos + 1)
            
        if len(fs_a) > cutoff:
            fs = (
                L+1,
                fs_a,
                fs_ap,
            )
            heapq.heappush(heap, (L+1, fs))
            yield fs

        if len(fs_c) > cutoff:
            fs = (
                L+1,
                fs_c,
                fs_cp,
            )
            heapq.heappush(heap, (L+1, fs))
            yield fs

        if len(fs_g) > cutoff:
            fs = (
                L+1,
                fs_g,
                fs_gp,
            )
            heapq.heappush(heap, (L+1, fs))
            yield fs

        if len(fs_t) > cutoff:
            fs = (
                L+1,
                fs_t,
                fs_tp,
            )
            heapq.heappush(heap, (L+1, fs))
            yield fs

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/build/lib/ArraySplitter/core_functions/tools/__init__.py</path>
<content>

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/build/lib/ArraySplitter/core_functions/tools/parsers.py</path>
<content>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright (C) 2007-2009 Aleksey Komissarov ( ad3002@gmail.com )
# All rights reserved.
#
# This software is licensed as described in the file COPYING, which
# you should have received as part of this distribution.
"""
Function for various parsing tasks.

- parse_fasta_head(fa_head) -> (P1,P2,P3)
- parse_chromosome_name(head) -> string
- trf_parse_line(line) -> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, '0', '0']
- trf_parse_param(line) -> string
- trf_parse_head(line) -> string  
"""
import re


def parse_fasta_head(fa_head):
    """Parsing fasta head.

    - head -> (...) -> head parameters
    - Head parsing <= fasta subtypes

    #TODO: Write other variants

    Valid examples of head format:

    - >gi|20928815|ref|NW_003237.1|MmUn_WIFeb01_12612 Mus musculus chromosome Un genomic contig
    - Short variant >(\d*)\t(\S*)
    - >134124-14124\n
    - >134124\n
    - >probe|misat|ref|CAAA01154094|start|991|end|1019
    - Repbase: >TC5A    Mariner/Tc1    Caenorhabditis elegans (split by \t <=== settings)
    - >lcl|HmaUn_WGA106_1 Hydra magnipapillata genomic contig, re...
    - plants: >gi|293886233|dbj|BABO01423189.1|
    """

    head_regexp = re.compile("^>?gi\|(\d+)\|(?:ref|dbj)\|([\w.]+)\|(.*)")
    regexp_obj = head_regexp.search(fa_head)

    head_regexp_lcl = re.compile("^>?lcl\|(.*?) (.*)")
    regexp_obj_lcl = head_regexp_lcl.search(fa_head)

    head_regexp_psu = re.compile("^>?psu\|(.*?) (.*)")
    regexp_obj_psu = head_regexp_psu.search(fa_head)

    head_regexp_short = re.compile("^>(\d+)\t(\S+)")
    regexp_obj_short = head_regexp_short.search(fa_head)

    head_regexp_comp = re.compile("^>(\d+)-(\d+)")
    regexp_obj_comp = head_regexp_comp.search(fa_head)

    head_regexp_number = re.compile("^>(\d+)")
    regexp_obj_number = head_regexp_number.search(fa_head)

    head_regexp_probe_index = re.compile(
        "^>probe\|(.*?)\|ref\|(.*?)\|start\|(\d*?)\|end\|(\d*)"
    )
    regexp_obj_probe_index = head_regexp_probe_index.search(fa_head)

    head_wgs = re.compile("^>?gi\|(\d+)\|\w+?\|([\w.]+)\|(.*)")
    regexp_obj_wgs = head_wgs.search(fa_head)

    head_trace = re.compile("^>gnl\|ti\|(\d+) (.*)")
    regexp_obj_trace = head_trace.search(fa_head)

    if regexp_obj:
        match = regexp_obj.groups()
        return list(match)
    elif regexp_obj_probe_index:
        match_l = []

        match = regexp_obj_probe_index.groups()

        gi = "%s_%s_%s" % (match[1], match[2], match[3])
        desc = "%s_%s_%s_%s" % (match[1], match[2], match[3], match[0])

        match_l.append(gi)
        match_l.append("None")
        match_l.append(desc)
        return list(match_l)
    elif regexp_obj_lcl:
        match_l = []
        match = regexp_obj_lcl.groups()
        match_l.append(match[0])
        match_l.append(match[0])
        match_l.append(match[1])
        return list(match_l)
    elif regexp_obj_psu:
        match_l = []
        match = regexp_obj_psu.groups()
        match_l.append(match[0])
        match_l.append(match[0])
        match_l.append(match[1])
        return list(match_l)
    elif regexp_obj_short:
        match = regexp_obj_short.groups()
        match = list(match)
        match.append("Unknown")
        return list(match)
    elif regexp_obj_comp:
        match = regexp_obj_comp.groups()
        match = list(match)
        match.append("Unknown")
        return list(match)
    elif regexp_obj_number:
        match = regexp_obj_number.groups()
        match = list(match)
        match.append("Unknown")
        match.append("Unknown")
        return list(match)
    elif regexp_obj_wgs:
        match = regexp_obj_wgs.groups()
        return list(match)
    elif regexp_obj_trace:
        match = list(regexp_obj_trace.groups())
        match.append(match[-1])
        return list(match)
    else:
        match = ("Unknown", "Unknown", "Unknown")
        # print "Failed parse sequence head: %s" % (fa_head)
        return list(match)


def parse_chromosome_name(head):
    """Parse chromosome name in ncbi fasta head"""
    # Head -> (...) -> Chromosome name or ""
    # TODO: write parse_chromosome_name function
    try:
        chr0 = re.compile("chromosome ([^, ]+)").findall(head)
        chr1 = re.compile("chromosome (\S+?),").findall(head)
        chr2 = re.compile("chromosome (\S+?),?").findall(head)
        chr3 = re.compile("chr(\S+?) ").findall(head)
        mit = re.compile(" (mitochon\S+?) ").findall(head)
        if chr0:
            return chr0[0]
        if chr1:
            return chr1[0]
        if chr2:
            return chr2[0]
        if chr3:
            return chr3[0]
        if mit:
            return "MT"
        return "?"
    except:
        return "?"


def trf_parse_line(line):
    """Parse TRF data line"""
    line = line.strip()
    groups = re.split("\s", line)

    if groups and len(groups) == 15:
        return list(groups)
    else:
        print(("Failed parse ta: %s" % (line)))
        return [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, "0", "0"]


def trf_parse_param(line):
    """TRF parameters line"""
    try:
        res = re.compile("Parameters: ([\d ]*)", re.S).findall(line)[0]
        return res
    except:
        res = "Unknown"
        print(("Failed parse param: %s" % (line)))
        return res


def trf_parse_head(line):
    """Parse TRF head"""
    try:
        res = re.compile("Sequence: (.*?)\n", re.S).findall(line)
        res2 = re.compile("Sequence: (.*)", re.S).findall(line)
        if res:
            return res[0]
        if res2:
            return res2[0]
    except:
        res = "Unknown"
        print(("Failed parse head: %s" % (line)))
        return res


def get_wgs_prefix_from_ref(ref):
    """Function parse WGS prefix from Genbank ref."""
    reg_exp = "([A-Z]+)"
    res = re.search(reg_exp, ref)
    if res:
        return res.group(0)
    else:
        return "UNKN"


def get_wgs_prefix_from_head(head):
    """Function parse WGS prefix from fasta head."""
    reg_exp = "ref.([A-Z]{4,})"
    res = re.search(reg_exp, head)
    if res:
        return res.group(1)
    else:
        reg_exp = "gb.([A-Z]{4,})"
        res = re.search(reg_exp, head)
        if res:
            return res.group(1)
        else:
            return None


def refine_name(i, trf_obj):
    """Refine TRF name
    :param trf_obj: TRF object
    :return: TRF object with refined name
    """
    name = trf_obj.trf_head.split()
    if len(name):
        name = name[0]
    else:
        name = name
    trf_obj.trf_id = f"{name}_{trf_obj.trf_l_ind}_{trf_obj.trf_r_ind}"
    trf_obj.id = f"AGT{(i+1) * 100:013d}"
    trf_obj.trf_consensus = trf_obj.trf_consensus.upper()
    trf_obj.trf_array = trf_obj.trf_array.upper()
    return trf_obj

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/build/lib/ArraySplitter/core_functions/tools/rotation.py</path>
<content>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# @created: 20.02.2024
# @author: Aleksey Komissarov
# @contact: ad3002@gmail.com

from .sequences import get_revcomp
from collections import Counter
from tqdm import tqdm

def best_kmer_for_start(arrays, k=5):
    k = 5
    kmers = Counter()
    for array in arrays:
        repeat = "".join(array)
        prev_kmer = None
        for i in range(len(repeat)-k+1):
            kmer = repeat[i:i+k]
            if kmer == prev_kmer:
                continue
            prev_kmer = kmer
            kmers[kmer] += 1

    best_kmer = None
    max_f1 = 0
    for kmer, _ in tqdm(kmers.most_common(100)):
        copy_number = []
        for array in arrays:
            for repeat in array:
                copy_number.append(repeat.count(kmer))

        n = len(copy_number)
        r = len([x for x in copy_number if x > 0])/n
        p = len([x for x in copy_number if x == 1])/n
        f1 = 2 * (r * p) / (r + p)
        if f1 < 0.5:
            continue
        if f1 > max_f1:
            best_kmer = (r, p, f1, kmer)
            max_f1 = f1
    return max_f1, best_kmer

def rotate_arrays(arrays_, starting_kmer=None):

    if not starting_kmer:
        arrays = []
        for array in arrays_:
            feature1 = array.count("C") < array.count("G")
            feature2 = array.count("A") < array.count("T")
            if feature1 and feature2 or (not feature1 and not feature2):
                if feature1 and feature2:
                    arrays.append(array.split())
                else:
                    arrays.append(get_revcomp(array).split())
            else:
                if feature1 and not feature2:
                    arrays.append(get_revcomp(array).split())
                else:
                    arrays.append(array.split())
        max_f1, (r, p, f1, best_kmer) = best_kmer_for_start(arrays)
    else:
        best_kmer = starting_kmer
        best_rev_kmer = get_revcomp(best_kmer)
        arrays = []
        for array in arrays_:
            forward_feature = array.count(best_rev_kmer) < array.count(best_kmer)
            if forward_feature:
                arrays.append(array.split())
            else:
                arrays.append(get_revcomp(array).split())
        
    new_arrays = []
    for array in arrays:
        new_array = []
        prev_repeat = ""
        queue = array[::]
        while queue:
            repeat = prev_repeat + queue.pop(0)
            pos = repeat.find(best_kmer, 1)
            if pos == -1:
                prev_repeat = repeat
                continue
            new_array.append(repeat[:pos])
            prev_repeat = repeat[pos:]
        if prev_repeat:
            while True:
                pos = prev_repeat.find(best_kmer, 1)
                if pos == -1:
                    new_array.append(prev_repeat)
                    break
                new_array.append(prev_repeat[:pos])
                prev_repeat = prev_repeat[pos:]
            
        assert "".join(array) == "".join(new_array)
        
        new_arrays.append(new_array)

    return new_arrays

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/build/lib/ArraySplitter/core_functions/io/fasta_reader.py</path>
<content>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# @created: 05.06.2011
# @author: Aleksey Komissarov
# @contact: ad3002@gmail.com

import gzip


def sc_iter_fasta_file(file_name, inmem=False, lower=False):
    """Iter over fasta file."""

    header = None
    seq = []
    if file_name.endswith(".gz"):
        opener = gzip.open
        decoder = lambda x: x.decode("utf8")
    else:
        opener = open
        decoder = lambda x: x

    with opener(file_name) as fh:
        if inmem:
            data = [decoder(x) for x in fh.readlines()]
            decoder = lambda x: x
        else:
            data = fh
        for line in data:
            line = decoder(line)
            if line.startswith(">"):
                if seq or header:
                    sequence = "".join(seq)
                    if lower:
                        sequence = sequence.lower()
                    yield header, sequence
                header = line.strip()[1:]
                seq = []
                continue
            seq.append(line.strip())
        if seq or header:
            sequence = "".join(seq)
            if lower:
                sequence = sequence.lower()
            yield header, sequence


def sc_iter_arrays_fasta_file(fasta_file):
    """Iter over fasta file and yield sequences."""
    for header, sequence in sc_iter_fasta_file(fasta_file):
        yield sequence

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/build/lib/ArraySplitter/core_functions/io/satellome_reader.py</path>
<content>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# @created: 20.02.2024
# @author: Aleksey Komissarov
# @contact: ad3002@gmail.com

from .tab_file import sc_iter_tab_file
from ..models.trf_model import TRModel


def sc_iter_arrays_satellome_file(trf_file):
    """Iter over satellome file."""
    for trf_obj in sc_iter_tab_file(trf_file, TRModel):
        yield trf_obj.trf_array

def sc_iter_satellome_file(trf_file):
    """Iter over satellome file."""
    for trf_obj in sc_iter_tab_file(trf_file, TRModel):
        yield trf_obj.trf_id, trf_obj.trf_array

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/build/lib/ArraySplitter/core_functions/io/__init__.py</path>
<content>

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/build/lib/ArraySplitter/core_functions/io/block_file.py</path>
<content>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# @created: 05.06.2011
# @author: Aleksey Komissarov
# @contact: ad3002@gmail.com

"""
Classes:
    
- AbstractBlockFileIO(AbstractFileIO)
    
"""

from PyExp import AbstractFileIO


class AbstractBlockFileIO(AbstractFileIO):
    """Working with file with data organized in block, where each block starts with same token.

    Public methods:

    - get_block_sequence(self, head_start, next_head, fh)
    - get_blocks(self, token, fh)
    - gen_block_sequences(self, token, fh)

    Inherited public properties:

    - data  - iterable data, each item is tuple (head, body)
    - N     - a number of items in data

    Overrided public methods:

    - __init__(self, token)
    - read_from_file(self, input_file)
    - read_online(self, input_file) ~> item

    Inherited public methods:

    - read_from_db(self, db_cursor)
    - write_to_file(self, output_file)
    - write_to_db(self, db_cursor)
    - read_as_iter(self, source)
    - iterate(self) ~> item of data
    - do(self, cf, args) -> result
    - process(self, cf, args)
    - clear(self)
    - do_with_iter(self, cf, args) -> [result,]
    - process_with_iter(self, cf, args)

    """

    def __init__(self, token, **args):
        """Overrided. Set token velue."""
        super(AbstractBlockFileIO, self).__init__(**args)
        self.token = token
        self.wise_opener = self.get_opener()

    def read_from_file(self, input_file):
        """Overrided. Read data from given input_file."""
        with self.wise_opener(input_file, "r") as fh:
            for head, body, start, next in self.gen_block_sequences(self.token, fh):
                self.data.append((head, body, start, next))

    def read_online(self, input_file):
        """Overrided. Yield items from data online from input_file."""
        with self.wise_opener(input_file, "r") as fh:
            for head, body, start, next in self.gen_block_sequences(self.token, fh):
                yield (head, body, start, next)

    def get_block_sequence(self, head_start, next_head, fh):
        """Get a data block (head, seq, head_start, head_end).

        Arguments:

        - head_start -- a head starting position in a file
        - next_head  -- a next to head starting position in a file
        - fh         -- an open file handler

        Return format:

        - head       -- a block head
        - seq        -- a block body
        - head_start -- a file pointer to block start
        - head_end   -- a file pointer to next block start or 0
        """
        head_start = int(head_start)
        next_head = int(next_head)
        sequence = ""
        fh.seek(head_start)
        pos = head_start
        if next_head:
            head = fh.readline()
            pos = fh.tell()
            while pos != next_head:
                temp_seq = fh.readline()
                pos = fh.tell()
                if pos != next_head:
                    sequence += temp_seq
            sequence += temp_seq
        else:
            head = fh.readline()
            fasta_list = fh.readlines()
            sequence = "".join(fasta_list)
        return (head, sequence, head_start, next_head)

    def get_blocks(self, token, fh):
        """Get a list of the token positions in given file (first, next).
        For the last string the function returns (last string, 0).

        Arguments:

        - token -- the token indicating a block start
        - fh    -- an open file handler

        Return format: a list of (start, next start) tuples
        """
        fh.seek(0)
        header_start_list = []
        here = 0
        wrong = 0
        start = -1
        while fh:
            here = fh.tell()
            line = fh.readline()
            if len(line) == 0:
                header_start_list.append((start, 0))
                break
            if line.startswith(token):
                if start == -1:
                    start = here
                else:
                    header_start_list.append((start, here))
                    start = here
        return header_start_list

    def gen_block_sequences(self, token, fh):
        """Yield (head, seq, head_start, head_end) tuplefor given fh for open file.

        Arguments:

        - token -- the token indicating a block start
        - fh    -- an open file handler

        Return format:

        - head       -- a block head
        - seq        -- a block body
        - head_start -- a file pointer to block start
        - head_end   -- a file pointer to next block start or 0

        """
        header_start_list = self.get_blocks(token, fh)
        for x, y in header_start_list:
            # yeild sequence by coordinates
            yield self.get_block_sequence(x, y, fh)

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/build/lib/ArraySplitter/core_functions/io/trf_reader.py</path>
<content>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# @created: 05.06.2011
# @author: Aleksey Komissarov
# @contact: ad3002@gmail.com

"""
Classes:

- TRFFileIO(AbstractBlockFileIO)

"""

from .block_file import AbstractBlockFileIO
from ..models.trf_model import TRModel


class TRFFileIO(AbstractBlockFileIO):
    """Working with raw output from TRF."""

    def __init__(self):
        """Overridden. Hardcoded start token."""
        token = "Sequence:"
        super(TRFFileIO, self).__init__(token)

    def iter_parse(self, trf_file, filter=True):
        """Iterate over raw trf data and yield TRFObjs.

        Args:
            trf_file (str): The path to the TRF file.
            filter (bool, optional): Whether to filter the TRF objects. Defaults to True.

        Yields:
            TRModel: The parsed TRF object.

        """
        for ii, (head, body, start, next) in enumerate(self.read_online(trf_file)):
            head = head.replace("\t", " ")
            obj_set = []
            n = body.count("\n")
            for i, line in enumerate(self._gen_data_line(body)):
                trf_obj = TRModel()
                trf_obj.set_raw_trf(head, None, line)
                obj_set.append(trf_obj)
            yield from obj_set

    def _gen_data_line(self, data):
        """Generate lines of data from the raw TRF data.

        Args:
            data (str): The raw TRF data.

        Yields:
            str: A line of data.

        """
        for line in data.split("\n"):
            line = line.strip()
            if line.startswith("Sequence"):
                continue
            if line.startswith("Parameters"):
                continue
            if not line:
                continue
            yield line


def sc_iter_arrays_trf_file(file_name):
    """Iter over trf file."""
    reader = TRFFileIO()
    for trf_obj in reader.iter_parse(file_name):
        yield trf_obj.trf_array

def sc_iter_trf_file(file_name):
    """Iter over trf file."""
    reader = TRFFileIO()
    for trf_obj in reader.iter_parse(file_name):
        yield trf_obj.trf_id, trf_obj.trf_array

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/build/lib/ArraySplitter/core_functions/io/tab_file.py</path>
<content>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# @created: 05.06.2011
# @author: Aleksey Komissarov
# @contact: ad3002@gmail.com
"""
Classes:
    
- TabDelimitedFileIO(AbstractFileIO)
    
"""
import csv
import os
import tempfile

from PyExp import AbstractFileIO

csv.field_size_limit(1000000000)


class TabDelimitedFileIO(AbstractFileIO):
    """Working with tab delimited file.

    Public methods:

    - sort(self, sort_func, reversed=False)

    Inherited public properties:

    - data  - iterable data, each item is tuple (head, body)
    - N     - a number of items in data

    Overrided public methods:

    - __init__(self, skip_first=False, format_func=None, delimeter="\\t", skip_startswith=None)
    - write_to_file(self, output_file)
    - read_from_file(self, input_file)
    - read_online(self, input_file) ~> item

    Inherited public methods:

    - read_from_db(self, db_cursor)
    - write_to_db(self, db_cursor)
    - read_as_iter(self, source)
    - iterate(self) ~> item of data
    - do(self, cf, args) -> result
    - process(self, cf, args)
    - clear(self)
    - do_with_iter(self, cf, args) -> [result,]
    - process_with_iter(self, cf, args)

    """

    def __init__(
        self, skip_first=False, format_func=None, delimeter="\t", skip_startswith="#"
    ):
        """Overrided. Set token velue.

        Keyword arguments:

        - skip_first   -- skip first line in file
        - format_funcs -- list of functions that format corresponding item in tab delimited line
        - delimeter    -- line delimeter
        - skip_startswith -- skip lines starts with this value
        """
        super(TabDelimitedFileIO, self).__init__()

        self.skip_first = skip_first
        self.format_func = format_func
        self.delimeter = delimeter
        self.skip_startswith = skip_startswith

    def read_from_file(self, input_file):
        """OVerrided. Read data from tab delimeted input_file."""
        with open(input_file) as fh:
            self._data = fh.readlines()
        if self.skip_first:
            self._data.pop(0)
        if self.skip_startswith:
            self._data = [
                line for line in self._data if not line.startswith(self.skip_startswith)
            ]
        self._data = [self._process_tab_delimeited_line(line) for line in self._data]

    def read_online(self, input_file):
        """Overrided. Yield items online from data from input_file."""
        with open(input_file) as fh:
            for i, line in enumerate(fh):
                if self.skip_first and i == 0:
                    continue
                if self.skip_startswith and line.startswith(self.skip_startswith):
                    continue
                yield self._process_tab_delimeited_line(line)

    def _process_tab_delimeited_line(self, line):
        """Format line with format_func."""

        line = line.strip().split(self.delimeter)
        if self.format_func:
            assert hasattr(self.format_func, "__call__")
            line = self.format_func(line)
        return line

    def _all_str(self, line):
        """Convert to string all items in line."""
        return [str(x) for x in line]

    def write_to_file(self, output_file):
        """Overrided. Write data to tab delimited output_file."""
        self._data = ["\t".join(self._all_str(line)) for line in self._data]
        with open(output_file, "w") as fh:
            fh.writelines(self._data)


def sc_iter_tab_file(
    input_file,
    data_type,
    skip_starts_with="#",
    remove_starts_with=None,
    preprocess_function=None,
    check_function=None,
):
    """Iter over tab file, yield an object of given data_type."""

    temp_file = tempfile.NamedTemporaryFile(delete=False)
    temp_file_name = temp_file.name

    if remove_starts_with:
        with open(input_file, "r") as fh:
            data = fh.readlines()
        data = [x for x in data if not x.startswith(remove_starts_with)]
        with open(temp_file_name, "w") as fh:
            fh.writelines(data)
        input_file = temp_file_name
    if preprocess_function:
        with open(input_file, "r") as fh:
            data = fh.readlines()
        data = [preprocess_function(x) for x in data]
        with open(temp_file_name, "w") as fh:
            fh.writelines(data)
        input_file = temp_file_name
    if check_function:
        with open(input_file, "r") as fh:
            data = fh.readlines()
        data = [x for x in data if check_function(x)]
        with open(temp_file_name, "w") as fh:
            fh.writelines(data)
        input_file = temp_file_name
    with open(input_file) as fh:
        fields = data_type().dumpable_attributes
        for data in csv.DictReader(
            fh, fieldnames=fields, delimiter="\t", quoting=csv.QUOTE_NONE
        ):
            if skip_starts_with:
                if data[fields[0]].startswith(skip_starts_with):
                    continue
            obj = data_type()
            obj.set_with_dict(data)
            yield obj
    if os.path.isfile(temp_file_name):
        os.unlink(temp_file_name)


def sc_iter_simple_tab_file(input_file):
    """Iter tab file, yield a list."""
    with open(input_file) as fh:
        for data in csv.reader(fh, delimiter="\t", quoting=csv.QUOTE_NONE):
            yield data


def sc_read_dictionary(dict_file, value_func=None):
    """Read file of tab-dilimited pairs.
    key\tvalue
    """
    result = {}
    with open(dict_file) as fh:
        for data in csv.reader(fh, delimiter="\t", quoting=csv.QUOTE_NONE):
            if hasattr(value_func, "__call__"):
                data[1] = value_func(data[1])
            result[data[0]] = data[1]
    return result


def sc_write_model_to_tab_file(output_file, objs):
    """Write model obj to tab-delimited file."""
    with open(output_file, "w") as fh:
        for obj in objs:
            fh.write(str(obj))


def sc_read_simple_tab_file(input_file, skip_first=False):
    """Iter tab file, yield a list."""
    result = []
    with open(input_file) as fh:
        if skip_first:
            fh.readline()
        for data in csv.reader(fh, delimiter="\t", quoting=csv.QUOTE_NONE):
            result.append(data)
    return result

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/build/lib/ArraySplitter/core_functions/models/__init__.py</path>
<content>

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/build/lib/ArraySplitter/core_functions/models/trf_model.py</path>
<content>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# @created: 14.06.2011
# @author: Aleksey Komissarov
# @contact: ad3002@gmail.com

import re

from PyExp import AbstractModel

from ..tools.parsers import (parse_chromosome_name,
                                                        parse_fasta_head,
                                                        trf_parse_head,
                                                        trf_parse_line)


def get_gc(sequence):
    """Count GC content."""
    length = len(sequence)
    if not length:
        return 0
    count_c = sequence.count("c") + sequence.count("C")
    count_g = sequence.count("g") + sequence.count("G")
    gc = float(count_c + count_g) / float(length)
    return float(gc)


def clear_sequence(sequence):
    """Clear sequence (full alphabet):

    - lower case
    - \s -> ""
    - [^actgn] -> ""
    """
    sequence = sequence.strip().upper()
    sequence = re.sub("\s+", "", sequence)
    return re.sub("[^actgnuwsmkrybdhvACTGNUWSMKRYBDHV\-]", "", sequence)


class TRModel(AbstractModel):
    """Class for tandem repeat wrapping

    Public properties:

    Indexes:

    - id,
    - giid,
    - trf_id,
    - project,
    - trf_gi

    Coordinates:

    - trf_l_ind,
    - trf_r_indel

    TR:

    - trf_period,
    - trf_n_copy,
    - trf_pmatch,
    - trf_pvar,
    - trf_entropy,
    - trf_array_length,
    - trf_joined,
    - trf_chr

    Sequence:

    - trf_consensus,
    - trf_array

    GC%:

    - trf_array_gc,
    - trf_consensus_gc

    Other:

    - trf_head,
    - trf_params

    Annotation:

    - trf_family,
    - trf_subfamily,
    - trf_subsubfamily,
    - trf_hor,
    - trf_n_chrun,
    - trf_n_refgenome,
    - trf_ref_annotation,
    - trf_bands_refgenome,
    - trf_repbase,
    - trf_strand

    Dumpable attributes:

    - "project",
    - "id" (int),
    - "trf_id" (int),
    - "trf_type",
    - "trf_family",
    - "trf_family_prob",
    - "trf_l_ind" (int),
    - "trf_r_ind" (int),
    - "trf_period" (int),
    - "trf_n_copy" (float),
    - "trf_pmatch" (float),
    - "trf_pvar" (float),
    - "trf_consensus",
    - "trf_array",
    - "trf_array_gc" (float),
    - "trf_consensus_gc" (float),
    - "trf_gi",
    - "trf_head",
    - "trf_param",
    - "trf_array_length" (int),
    - "trf_chr",
    - "trf_joined" (int),
    - "trf_superfamily",
    - "trf_superfamily_ref",
    - "trf_superfamily_self",
    - "trf_subfamily",
    - "trf_subsubfamily",
    - "trf_family_network",
    - "trf_family_self",
    - "trf_family_ref",
    - "trf_hor" (int),
    - "trf_n_chrun" (int),
    - "trf_ref_annotation",
    - "trf_bands_refgenome",
    - "trf_repbase",
    - "trf_strand",

    """

    dumpable_attributes = [
        "project",
        "id",
        "trf_id",
        "trf_type",
        "trf_family",
        "trf_family_prob",
        "trf_l_ind",
        "trf_r_ind",
        "trf_period",
        "trf_n_copy",
        "trf_pmatch",
        "trf_pvar",
        "trf_entropy",
        "trf_consensus",
        "trf_array",
        "trf_array_gc",
        "trf_consensus_gc",
        "trf_gi",
        "trf_head",
        "trf_param",
        "trf_array_length",
        "trf_chr",
        "trf_joined",
        "trf_superfamily",
        "trf_superfamily_ref",
        "trf_superfamily_self",
        "trf_subfamily",
        "trf_subsubfamily",
        "trf_family_network",
        "trf_family_self",
        "trf_family_ref",
        "trf_hor",
        "trf_n_chrun",
        "trf_ref_annotation",
        "trf_bands_refgenome",
        "trf_repbase",
        "trf_strand",
    ]

    int_attributes = [
        "trf_l_ind",
        "trf_r_ind",
        "trf_period",
        "trf_array_length",
        "trf_joined",
        "trf_hor",
        "trf_n_chrun",
    ]

    float_attributes = [
        "trf_n_copy",
        "trf_family_prob",
        "trf_entropy",
        "trf_pmatch",
        "trf_pvar",
        "trf_array_gc",
        "trf_consensus_gc",
    ]

    def set_project_data(self, project):
        """Add project data to self.project."""
        self.project = project

    def set_raw_trf(self, head, body, line):
        """Init object with data from parsed trf ouput.

        Parameters:

        - trf_obj: TRFObj instance
        - head: parsed trf head
        - body: parsed trf body
        - line: parsed trf line
        """
        self.trf_param = 0
        self.trf_head = trf_parse_head(head).strip()
        self.trf_gi = parse_fasta_head(self.trf_head)[0]
        self.trf_chr = parse_chromosome_name(self.trf_head)

        (
            self.trf_l_ind,
            self.trf_r_ind,
            self.trf_period,
            self.trf_n_copy,
            self.trf_l_cons,
            self.trf_pmatch,
            self.trf_indels,
            self.trf_score,
            self.trf_n_a,
            self.trf_n_c,
            self.trf_n_g,
            self.trf_n_t,
            self.trf_entropy,
            self.trf_consensus,
            self.trf_array,
        ) = trf_parse_line(line)

        self.trf_pmatch = float(self.trf_pmatch)
        self.trf_pvar = int(100 - float(self.trf_pmatch))

        try:
            self.trf_l_ind = int(self.trf_l_ind)
        except:
            print("Error:", self)

        self.trf_r_ind = int(self.trf_r_ind)
        self.trf_period = int(self.trf_period)
        self.trf_n_copy = float(self.trf_n_copy)

        self.trf_consensus = clear_sequence(self.trf_consensus)
        self.trf_array = clear_sequence(self.trf_array)

        self.trf_array_gc = get_gc(self.trf_array)
        self.trf_consensus_gc = get_gc(self.trf_consensus)
        self.trf_chr = parse_chromosome_name(self.trf_head)
        self.trf_array_length = len(self.trf_array)

    def get_header_string(self):
        """Get header string for tsv file."""
        data = "\t".join(self.dumpable_attributes)
        return f"#{data}\n"

    def get_fasta_repr(self, add_project=False):
        """Get array fasta representation, head - trf_id."""
        if add_project:
            return ">%s_%s\n%s\n" % (self.trf_id, self.project, self.trf_array)
        else:
            return ">%s\n%s\n" % (self.trf_id, self.trf_array)

    @property
    def fasta(self):
        return self.get_fasta_repr()

    def get_gff3_string(
        self,
        chromosome=True,
        trs_type="complex_tandem_repeat",
        probability=1000,
        tool="PySatDNA",
        prefix=None,
        properties=None,
        force_header=False,
    ):
        """Return TR in gff format."""
        if chromosome and self.trf_chr and self.trf_chr != "?":
            seqid = self.trf_chr
        elif self.trf_gi and self.trf_gi != "Unknown":
            seqid = self.trf_gi
        else:
            seqid = self.trf_head
        features = []
        if not properties:
            properties = {}
        for name, attr in properties.items():
            features.append("%s=%s" % (name, getattr(self, attr)))
        features = ";".join(features)
        if prefix:
            seqid = prefix + seqid
        if self.trf_l_ind < self.trf_r_ind:
            strand = "+"
        else:
            strand = "-"
            self.trf_l_ind, self.trf_r_ind = self.trf_r_ind, self.trf_l_ind

        if force_header:
            seqid = self.trf_head
        d = (
            seqid,
            tool,
            trs_type,
            self.trf_l_ind,
            self.trf_r_ind,
            probability,
            strand,
            ".",
            features,
        )
        return "%s\n" % "\t".join(map(str, d))

    def get_bed_string(self):
        """Return TR in bed format."""
        if self.trf_l_ind < self.trf_r_ind:
            strand = "+"
        else:
            strand = "-"
            self.trf_l_ind, self.trf_r_ind = self.trf_r_ind, self.trf_l_ind

        seqid = self.trf_head
        d = (
            seqid,
            self.trf_l_ind,
            self.trf_r_ind,
        )
        return "%s\n" % "\t".join(map(str, d))

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/src/ArraySplitter/decompose.py</path>
<content>



import argparse

import os
from collections import Counter
import re
from tqdm import tqdm

import editdistance as ed

from .core_functions.io.fasta_reader import \
    sc_iter_fasta_file
from .core_functions.io.satellome_reader import \
    sc_iter_satellome_file
from .core_functions.io.trf_reader import sc_iter_trf_file
from .core_functions.tools.fs_tree import \
    iter_fs_tree_from_sequence


def get_top1_nucleotide(array):
    ### Step 1. Find the most frequent nucleotide (TODO: check all nucleotides and find with the best final score
    c = Counter()
    for n in "ACTG":
        c[n] = array.count(n)
        # print(n, array.count(n))
    return c.most_common(1)[0][0]


def get_fs_tree(array, top1_nucleotide, cutoff):
    ### Step 2. Build fs_tree (TODO:  optimize it for long sequences)
    names_ = [i for i in range(len(array)) if array[i] == top1_nucleotide]
    positions_ = names_[::]
    # print(f"Starting positions: {len(positions_)}")
    return iter_fs_tree_from_sequence(
        array, top1_nucleotide, names_, positions_, cutoff
    )


def iterate_hints(array, fs_tree, depth):
    ### Step 3. Find a list of hints (hint is the sequenece for array cutoff)

    current_length = 0
    buffer = []
    for L, names, positions in fs_tree:
        if L != current_length:
            if buffer:
                max_n = 0
                found_seq = None
                for start, end, N in buffer:
                    if N > max_n:
                        max_n = N
                        found_seq = array[start : end + 1]
                yield current_length, found_seq, max_n
            buffer = []
            current_length = L
            if current_length > depth:
                break
        start = names[0]
        end = positions[0]
        N = len(names)
        buffer.append((start, end, N))
    if buffer:
        max_n = 0
        found_seq = None
        for start, end, N in buffer:
            if N > max_n:
                max_n = N
                found_seq = array[start : end + 1]
        yield current_length, found_seq, max_n


def compute_cuts(array, hints):
    ### Step 4. Find optimal cutoff
    best_cut_seq = None
    best_cut_score = 0.0
    best_period = None
    possible_solutions = Counter()
    solution2meta = {}
    for L, cut_sequence, N in hints:
        period2freq = Counter()
        for seq in array.split(cut_sequence):
            period2freq[len(seq + cut_sequence)] += 1
        period, period_freq = period2freq.most_common(1)[0]
        total_periods = sum(period2freq.values())
        score = period_freq / total_periods

        possible_solutions[period] += 1
        if not period in solution2meta:
            solution2meta[period] = [cut_sequence, score, period]
        else:
            if score > solution2meta[period][1]:
                solution2meta[period] = [cut_sequence, score, period]
        if score > best_cut_score:
            best_cut_score = score
            best_cut_seq = cut_sequence
            best_period = period
        # print(L, period, period_freq, score, possible_solutions, solution2meta)
    if not possible_solutions:
        return array, 0, len(array)
    best_period = possible_solutions.most_common(1)[0][0]
    best_cut_seq, best_cut_score, best_period = solution2meta[best_period]
    return best_cut_seq, best_cut_score, best_period


### Step 5a. Try to cut long monomers to expected
def refine_repeat_even(repeat, best_period):
    if len(repeat) % best_period == 0:
        start = 0
        for _ in range(len(repeat) // best_period):
            yield repeat[start : start + best_period]
            start += best_period
    else:
        yield repeat


def decompose_array_iter1(array, best_cut_seq, best_period, verbose=True):
    repeats2count = Counter()
    decomposition = []
    cuts = array.split(best_cut_seq)
    for ii, x in enumerate(cuts):
        if ii > 0:
            repeat = best_cut_seq + x
        elif not x:
            repeat = best_cut_seq
        else:
            repeat = x
        if not repeat:
            continue
        if len(repeat) == best_period:
            if verbose:
                print(len(repeat), repeat)
            repeats2count[repeat] += 1
            decomposition.append(repeat)
        else:
            # print(len(repeat), best_period)
            for repeat in refine_repeat_even(repeat, best_period):
                if verbose:
                    print(len(repeat), repeat)
                repeats2count[repeat] += 1
                decomposition.append(repeat)
    return decomposition, repeats2count


### Step 5b. Try to cut long monomers to expected
def refine_repeat_odd(repeat, best_period, most_common_monomer, verbose=False):
    if len(repeat) / best_period > 1.3:
        n = len(most_common_monomer)
        optimal_cut = 0
        best_ed = n

        begin_positions = [i for i in range(min(len(repeat) - n + 1, 5))]
        end_positions = [i for i in range(max(0, len(repeat) - n + 1 - 5), len(repeat) - n + 1)]

        for i in begin_positions+end_positions:
            rep_b = repeat[i : i + n]
            dist = ed.eval(most_common_monomer, rep_b)
            if dist < best_ed:
                best_ed = dist
                optimal_cut = i
                if verbose:
                    print(
                        "Optimal cut",
                        best_ed,
                        optimal_cut,
                        len(repeat[:optimal_cut]),
                        len(repeat[optimal_cut:]),
                    )
        if best_ed < n / 2:
            if optimal_cut == 0:
                optimal_cut += n
            a = repeat[:optimal_cut]
            b = repeat[optimal_cut:]
            if min(len(a), len(b)) < 0:  # n/3:
                yield repeat
            else:
                if a:
                    yield a
                if b:
                    yield b
        else:
            yield repeat
    else:
        yield repeat


def decompose_array_iter2(decomposition, best_period, repeats2count_ref, verbose=True):
    repeats2count = Counter()
    refined_decomposition = []
    most_common_monomer = None
    for monomer, tf in repeats2count_ref.most_common(1000):
        if len(monomer) == best_period:
            most_common_monomer = monomer
            break
    assert most_common_monomer
    for repeat in decomposition:
        if verbose:
            print("Repeat under consideration", len(repeat), repeat)
        for repeat in refine_repeat_odd(
            repeat, best_period, most_common_monomer, verbose=verbose
        ):
            if verbose:
                print("Added:", len(repeat), repeat)
            repeats2count[repeat] += 1
            refined_decomposition.append(repeat)
    return (
        refined_decomposition,
        repeats2count,
        len(refined_decomposition) != len(decomposition),
    )


def print_monomers(decomposition, repeats2count, best_period):
    start2tf = Counter()
    for monomer in decomposition:
        start2tf[monomer[:5]] += 1
    print(start2tf)

    most_common_monomer = None
    for monomer, tf in repeats2count.most_common(1000):
        if len(monomer) == best_period:
            most_common_monomer = monomer
            break
    assert most_common_monomer
    for repeat in decomposition:
        print(
            len(repeat),
            start2tf[repeat[:5]],
            repeat,
            ed.eval(repeat, most_common_monomer),
        )


def print_pause_clean(decomposition, repeats2count, best_period):
    print_monomers(decomposition, repeats2count, best_period)
    input("?")


#   clear_output(wait=True)


def decompose_array(array, depth=500, cutoff=20, verbose=False):
    ### Step 1. Find the most frequent nucleotide (TODO: check all nucleotides and find the one with the best final score)
    top1_nucleotide = get_top1_nucleotide(array)
    # print("top1_nucleotide:", top1_nucleotide)
    # top1_nucleotide = "A"
    ### Step 2. Build fs_tree (TODO: optimize it for long sequences)
    fs_tree = get_fs_tree(array, top1_nucleotide, cutoff=cutoff)
    ### Step 3. Find a list of hints (hint is the sequence for array cutting)
    ### Here I defined it as a sequence with maximal coverage in the original array for a given length
    hints = iterate_hints(array, fs_tree, depth)
    ### Step 4. Find the optimal cut sequence and the best period
    ### Defined as the maximal fraction of the cut sequence to the total cut sequence
    best_cut_seq, best_cut_score, best_period = compute_cuts(array, hints)

    ### Step 5. Cut the array
    ### The first iteration finds monomer frequencies
    # print("Firset iteration")
    decomposition, repeats2count = decompose_array_iter1(
        array, best_cut_seq, best_period, verbose=verbose
    )
    
    # assert "".join(decomposition) == array
    ### The second iteration tries to cut longer monomers to the expected length
    changed = True
    while changed:
        # print("Firset iteration", len(decomposition))
        decomposition, repeats2count, changed = decompose_array_iter2(
            decomposition, best_period, repeats2count, verbose=verbose
        )
        # assert "".join(decomposition) == array

    ### TODO: The third iteration tries to glue short dangling monomers to the nearest monomer

    return decomposition, repeats2count, best_cut_seq, best_cut_score, best_period


def get_array_generator(input_file, format):
    '''Get array generator by format.'''
    if format == "fasta":
        return sc_iter_fasta_file(input_file)
    if format == "trf":
        return sc_iter_trf_file(input_file)
    if format == "satellome":
        return sc_iter_satellome_file(input_file)
    
    print(f"Unknown format: {format}")
    exit(1)
    

def main(input_file, output_prefix, format, threads):
    """Main function."""

    sequences = get_array_generator(input_file, format)
    total = 0
    for _ in sequences:
        total += 1
    sequences = get_array_generator(input_file, format)

    print(f"Start processing")
    

    depth = 100
    cutoff = None
    verbose = False

    if output_prefix.endswith(".fasta"):
        print("Remove .fasta from output prefix")
        output_prefix = output_prefix[:-6]
    elif output_prefix.endswith(".fa"):
        print("Remove .fa from output prefix")
        output_prefix = output_prefix[:-3]

    output_file = f"{output_prefix}.decomposed.fasta"
    print(f"Output file: {output_file}")
    
    with open(output_file, "w") as fw:
        for header, array in tqdm(sequences, total=total):
            # print(len(array), end=" ")
            if not cutoff:
                if len(array) > 1_000_000:
                    cutoff = 1000
                elif len(array) > 100_000:
                    cutoff = 250
                elif len(array) > 10_000:
                    cutoff = 10
                else:
                    cutoff = 3
            (
                decomposition,
                repeats2count,
                best_cut_seq,
                best_cut_score,
                best_period,
            ) = decompose_array(array, depth=depth, cutoff=cutoff, verbose=verbose)

            # print("best period:", best_period, "len:", len(decomposition))
            # print_pause_clean(decomposition, repeats2count, best_period)

            fw.write(f">{header} {best_period}\n")
            fw.write(" ".join(decomposition) + "\n")


def run_it():
    parser = argparse.ArgumentParser(
        description="De novo decomposition of satellite DNA arrays into monomers"
    )
    parser.add_argument("-i", "--input", help="Input file", required=True)
    parser.add_argument(
        "--format",
        help="Input format: fasta, trf [fasta]",
        required=False,
        default="fasta",
    )
    parser.add_argument("-o", "--output", help="Output prefix", required=True)
    parser.add_argument(
        "-t", "--threads", help="Number of threads", required=False, default=4
    )
    args = parser.parse_args()

    input_file = args.input
    output_prefix = args.output
    format = args.format
    threads = int(args.threads)

    if not os.path.isfile(input_file):
        print(f"File {input_file} not found")
        exit(1)

    main(input_file, output_prefix, format, threads)

if __name__ == "__main__":
    run_it()
</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/src/ArraySplitter/__init__.py</path>
<content>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# @created: 20.02.2024
# @author: Aleksey Komissarov
# @contact: ad3002@gmail.com

__version__ = "1.2.3"

# __all__ = []

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/src/ArraySplitter/rotate.py</path>
<content>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# @created: 21.02.2024
# @author: Aleksey Komissarov
# @contact: ad3002@gmail.com

import argparse

from .core_functions.tools.rotation import rotate_arrays

from .core_functions.io.fasta_reader import sc_iter_fasta_file

def main(fasta_file, output_file, starting_kmer=None):

    arrays = []
    for ii, (header, seq) in enumerate(sc_iter_fasta_file(fasta_file)):
        arrays.append(seq)

    new_arrays = rotate_arrays(arrays, starting_kmer=starting_kmer)
    
    with open(output_file, "w") as fw:
        for ii, (header, seq) in enumerate(sc_iter_fasta_file(fasta_file)):
            fw.write(f'>{header}\n{" ".join(new_arrays[ii])}\n')
            
def run_it():
    parser = argparse.ArgumentParser(description='Rotate arrays to start from the same position.')
    parser.add_argument('-i', '--fasta', type=str, help='Fasta file', required=True)
    parser.add_argument('-o', '--output', type=str, help='Output file', required=True)
    parser.add_argument('-s', '--start', type=str, help='Starting kmer for rotation [None]', default=None)
    args = parser.parse_args()
    main(args.fasta, args.output, starting_kmer=args.start)

if __name__ == '__main__':
    run_it()
</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/src/ArraySplitter/extract.py</path>
<content>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# @created: 21.02.2024
# @author: Aleksey Komissarov
# @contact: ad3002@gmail.com

from collections import Counter
import argparse
import logging
from .core_functions.io.fasta_reader import sc_iter_fasta_file

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def main(fasta_file, output_prefix, distance=0, function='hd'):

    if distance < 0:
        raise ValueError(f'Distance should be >= 0, but {distance} given')
    
    if function not in ['hd', 'ed']:
        raise ValueError(f'Function should be hd or ed, but {function} given')
    
    logging.info(f'Extract monomers from {fasta_file}')
    monomers = Counter()
    for header, seq in sc_iter_fasta_file(fasta_file):
        for monomer in seq.split():
            monomers[monomer] += 1

    logging.info(f'Found {len(monomers)} uniq monomers with total {sum(monomers.values())} monomers')

    logging.info(f'Filter monomers with distance <= {distance} using {function} function')
    # if function == 'hd':
    #     monomers = {k: v for k, v in monomers.items() if v <= distance}
    # else:
    #     raise NotImplementedError('Edit distance is not implemented yet')
    logging.info(f'Found {len(monomers)} monomers with distance <= {distance}')

    output_file = f'{output_prefix}.monomers.tsv'
    logging.info(f'Save monomers to {output_file}')
    with open(output_file, "w") as fw:
        for monomer, count in monomers.most_common(1000000000):
            fw.write(f'{count}\t{len(monomer)}\t{monomer}\n')
            
def run_it():
    parser = argparse.ArgumentParser(description='Extract monomers from array over edit distance.')
    parser.add_argument('-i', '--fasta', type=str, help='Fasta file with arrays', required=True)
    parser.add_argument('-o', '--output', type=str, help='Output prefix', required=True)
    parser.add_argument('-f', '--function', type=str, help='Distance function hamming (hd) or edit (ed) [hd]', default='hd')
    parser.add_argument('-d', '--distance', type=int, help='Distance value, you will get monomers over <= distance [0]', default=0)
    args = parser.parse_args()
    main(args.fasta, args.output, distance=args.distance, function=args.function)

if __name__ == '__main__':
    run_it()
</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/src/ArraySplitter/core_functions/__init__.py</path>
<content>

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/src/ArraySplitter/core_functions/tools/sequences.py</path>
<content>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# @created: 20.02.2024
# @author: Aleksey Komissarov
# @contact: ad3002@gmail.com


REVCOMP_DICTIONARY = dict(list(zip("ATCGNatcgn~[] ", "TAGCNtagcn~][ ")))

def get_revcomp(sequence):
    """Return reverse complementary sequence.

    >>> complementary('AT CG')
    'CGAT'

    """
    return "".join(
        REVCOMP_DICTIONARY.get(nucleotide, "") for nucleotide in reversed(sequence)
    )

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/src/ArraySplitter/core_functions/tools/fs_tree.py</path>
<content>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# @created: 02.02.2024
# @author: Aleksey Komissarov
# @contact: ad3002@gmail.com

from collections import deque

import heapq

class WeightedValueHeap:
    def __init__(self):
        self.heap = []
    
    def insert(self, weight, value):
        heapq.heappush(self.heap, (weight, value))
    
    def pop(self):
        if self.heap:
            return heapq.heappop(self.heap)
        return None
    
    def peek(self):
        if self.heap:
            return self.heap[0]
        return None

def update(
    fs_tree,
    queue,
    nucl,
    fs_x,
    fs_xp,
    current_cid,
    cid,
    cutoff,
):
    if len(fs_x) > cutoff:
        fs_tree[cid] = (
            cid,
            [nucl],
            fs_x,
            fs_xp,
            None,
            [],
        )
        fs_tree[current_cid][5].append(cid)
        queue.append(cid)
        cid += 1
    return cid


def build_fs_tree_from_sequence(array, starting_seq_, names_, positions_, cutoff):
    fs_tree = {}
    queue = []
    cid = 0
    seq = starting_seq_
    names = names_[::]
    positions = positions_[::]
    children = []
    fs = (cid, [seq], names, positions, None, children)
    queue.append(cid)
    fs_tree[cid] = fs
    cid += 1
    cutoff = cutoff

    while queue:
        qcid = queue.pop()

        fs = fs_tree[qcid]

        fs_a, fs_c, fs_g, fs_t = [], [], [], []
        fs_ap, fs_cp, fs_gp, fs_tp = [], [], [], []

        for ii, pos in enumerate(fs[3]):
            current_cid = fs[0]
            seq = fs[1]
            name = fs[2][ii]

            if pos + 1 == len(array):
                continue

            nucl = array[pos + 1]
            if nucl == "A":
                fs_a.append(name)
                fs_ap.append(pos + 1)
            elif nucl == "C":
                fs_c.append(name)
                fs_cp.append(pos + 1)
            elif nucl == "G":
                fs_g.append(name)
                fs_gp.append(pos + 1)
            elif nucl == "T":
                fs_t.append(name)
                fs_tp.append(pos + 1)
            
        cid = update(
            fs_tree,
            queue,
            "A",
            fs_a,
            fs_ap,
            current_cid,
            cid,
            cutoff,
        )
        cid = update(
            fs_tree,
            queue,
            "C",
            fs_c,
            fs_cp,
            current_cid,
            cid,
            cutoff,
        )
        cid = update(
            fs_tree,
            queue,
            "G",
            fs_g,
            fs_gp,
            current_cid,
            cid,
            cutoff,
        )
        cid = update(
            fs_tree,
            queue,
            "T",
            fs_t,
            fs_tp,
            current_cid,
            cid,
            cutoff,
        )

    return fs_tree


def iter_fs_tree_from_sequence(array, starting_seq_, names_, positions_, cutoff):
    heap = []
    names = names_[::]
    positions = positions_[::]
    fs = (1, names, positions)
    heapq.heappush(heap, (1, fs))
    cutoff = cutoff
    while heap:
        L, fs = heapq.heappop(heap)

        fs_a, fs_c, fs_g, fs_t = [], [], [], []
        fs_ap, fs_cp, fs_gp, fs_tp = [], [], [], []

        for ii, pos in enumerate(fs[2]):
            
            name = fs[1][ii]

            if pos + 1 == len(array):
                continue

            nucl = array[pos + 1]
            if nucl == "A":
                fs_a.append(name)
                fs_ap.append(pos + 1)
            elif nucl == "C":
                fs_c.append(name)
                fs_cp.append(pos + 1)
            elif nucl == "G":
                fs_g.append(name)
                fs_gp.append(pos + 1)
            elif nucl == "T":
                fs_t.append(name)
                fs_tp.append(pos + 1)
            
        if len(fs_a) > cutoff:
            fs = (
                L+1,
                fs_a,
                fs_ap,
            )
            heapq.heappush(heap, (L+1, fs))
            yield fs

        if len(fs_c) > cutoff:
            fs = (
                L+1,
                fs_c,
                fs_cp,
            )
            heapq.heappush(heap, (L+1, fs))
            yield fs

        if len(fs_g) > cutoff:
            fs = (
                L+1,
                fs_g,
                fs_gp,
            )
            heapq.heappush(heap, (L+1, fs))
            yield fs

        if len(fs_t) > cutoff:
            fs = (
                L+1,
                fs_t,
                fs_tp,
            )
            heapq.heappush(heap, (L+1, fs))
            yield fs

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/src/ArraySplitter/core_functions/tools/__init__.py</path>
<content>

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/src/ArraySplitter/core_functions/tools/parsers.py</path>
<content>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright (C) 2007-2009 Aleksey Komissarov ( ad3002@gmail.com )
# All rights reserved.
#
# This software is licensed as described in the file COPYING, which
# you should have received as part of this distribution.
"""
Function for various parsing tasks.

- parse_fasta_head(fa_head) -> (P1,P2,P3)
- parse_chromosome_name(head) -> string
- trf_parse_line(line) -> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, '0', '0']
- trf_parse_param(line) -> string
- trf_parse_head(line) -> string  
"""
import re


def parse_fasta_head(fa_head):
    """Parsing fasta head.

    - head -> (...) -> head parameters
    - Head parsing <= fasta subtypes

    #TODO: Write other variants

    Valid examples of head format:

    - >gi|20928815|ref|NW_003237.1|MmUn_WIFeb01_12612 Mus musculus chromosome Un genomic contig
    - Short variant >(\d*)\t(\S*)
    - >134124-14124\n
    - >134124\n
    - >probe|misat|ref|CAAA01154094|start|991|end|1019
    - Repbase: >TC5A    Mariner/Tc1    Caenorhabditis elegans (split by \t <=== settings)
    - >lcl|HmaUn_WGA106_1 Hydra magnipapillata genomic contig, re...
    - plants: >gi|293886233|dbj|BABO01423189.1|
    """

    head_regexp = re.compile("^>?gi\|(\d+)\|(?:ref|dbj)\|([\w.]+)\|(.*)")
    regexp_obj = head_regexp.search(fa_head)

    head_regexp_lcl = re.compile("^>?lcl\|(.*?) (.*)")
    regexp_obj_lcl = head_regexp_lcl.search(fa_head)

    head_regexp_psu = re.compile("^>?psu\|(.*?) (.*)")
    regexp_obj_psu = head_regexp_psu.search(fa_head)

    head_regexp_short = re.compile("^>(\d+)\t(\S+)")
    regexp_obj_short = head_regexp_short.search(fa_head)

    head_regexp_comp = re.compile("^>(\d+)-(\d+)")
    regexp_obj_comp = head_regexp_comp.search(fa_head)

    head_regexp_number = re.compile("^>(\d+)")
    regexp_obj_number = head_regexp_number.search(fa_head)

    head_regexp_probe_index = re.compile(
        "^>probe\|(.*?)\|ref\|(.*?)\|start\|(\d*?)\|end\|(\d*)"
    )
    regexp_obj_probe_index = head_regexp_probe_index.search(fa_head)

    head_wgs = re.compile("^>?gi\|(\d+)\|\w+?\|([\w.]+)\|(.*)")
    regexp_obj_wgs = head_wgs.search(fa_head)

    head_trace = re.compile("^>gnl\|ti\|(\d+) (.*)")
    regexp_obj_trace = head_trace.search(fa_head)

    if regexp_obj:
        match = regexp_obj.groups()
        return list(match)
    elif regexp_obj_probe_index:
        match_l = []

        match = regexp_obj_probe_index.groups()

        gi = "%s_%s_%s" % (match[1], match[2], match[3])
        desc = "%s_%s_%s_%s" % (match[1], match[2], match[3], match[0])

        match_l.append(gi)
        match_l.append("None")
        match_l.append(desc)
        return list(match_l)
    elif regexp_obj_lcl:
        match_l = []
        match = regexp_obj_lcl.groups()
        match_l.append(match[0])
        match_l.append(match[0])
        match_l.append(match[1])
        return list(match_l)
    elif regexp_obj_psu:
        match_l = []
        match = regexp_obj_psu.groups()
        match_l.append(match[0])
        match_l.append(match[0])
        match_l.append(match[1])
        return list(match_l)
    elif regexp_obj_short:
        match = regexp_obj_short.groups()
        match = list(match)
        match.append("Unknown")
        return list(match)
    elif regexp_obj_comp:
        match = regexp_obj_comp.groups()
        match = list(match)
        match.append("Unknown")
        return list(match)
    elif regexp_obj_number:
        match = regexp_obj_number.groups()
        match = list(match)
        match.append("Unknown")
        match.append("Unknown")
        return list(match)
    elif regexp_obj_wgs:
        match = regexp_obj_wgs.groups()
        return list(match)
    elif regexp_obj_trace:
        match = list(regexp_obj_trace.groups())
        match.append(match[-1])
        return list(match)
    else:
        match = ("Unknown", "Unknown", "Unknown")
        # print "Failed parse sequence head: %s" % (fa_head)
        return list(match)


def parse_chromosome_name(head):
    """Parse chromosome name in ncbi fasta head"""
    # Head -> (...) -> Chromosome name or ""
    # TODO: write parse_chromosome_name function
    try:
        chr0 = re.compile("chromosome ([^, ]+)").findall(head)
        chr1 = re.compile("chromosome (\S+?),").findall(head)
        chr2 = re.compile("chromosome (\S+?),?").findall(head)
        chr3 = re.compile("chr(\S+?) ").findall(head)
        mit = re.compile(" (mitochon\S+?) ").findall(head)
        if chr0:
            return chr0[0]
        if chr1:
            return chr1[0]
        if chr2:
            return chr2[0]
        if chr3:
            return chr3[0]
        if mit:
            return "MT"
        return "?"
    except:
        return "?"


def trf_parse_line(line):
    """Parse TRF data line"""
    line = line.strip()
    groups = re.split("\s", line)

    if groups and len(groups) == 15:
        return list(groups)
    else:
        print(("Failed parse ta: %s" % (line)))
        return [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, "0", "0"]


def trf_parse_param(line):
    """TRF parameters line"""
    try:
        res = re.compile("Parameters: ([\d ]*)", re.S).findall(line)[0]
        return res
    except:
        res = "Unknown"
        print(("Failed parse param: %s" % (line)))
        return res


def trf_parse_head(line):
    """Parse TRF head"""
    try:
        res = re.compile("Sequence: (.*?)\n", re.S).findall(line)
        res2 = re.compile("Sequence: (.*)", re.S).findall(line)
        if res:
            return res[0]
        if res2:
            return res2[0]
    except:
        res = "Unknown"
        print(("Failed parse head: %s" % (line)))
        return res


def get_wgs_prefix_from_ref(ref):
    """Function parse WGS prefix from Genbank ref."""
    reg_exp = "([A-Z]+)"
    res = re.search(reg_exp, ref)
    if res:
        return res.group(0)
    else:
        return "UNKN"


def get_wgs_prefix_from_head(head):
    """Function parse WGS prefix from fasta head."""
    reg_exp = "ref.([A-Z]{4,})"
    res = re.search(reg_exp, head)
    if res:
        return res.group(1)
    else:
        reg_exp = "gb.([A-Z]{4,})"
        res = re.search(reg_exp, head)
        if res:
            return res.group(1)
        else:
            return None


def refine_name(i, trf_obj):
    """Refine TRF name
    :param trf_obj: TRF object
    :return: TRF object with refined name
    """
    name = trf_obj.trf_head.split()
    if len(name):
        name = name[0]
    else:
        name = name
    trf_obj.trf_id = f"{name}_{trf_obj.trf_l_ind}_{trf_obj.trf_r_ind}"
    trf_obj.id = f"AGT{(i+1) * 100:013d}"
    trf_obj.trf_consensus = trf_obj.trf_consensus.upper()
    trf_obj.trf_array = trf_obj.trf_array.upper()
    return trf_obj

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/src/ArraySplitter/core_functions/tools/rotation.py</path>
<content>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# @created: 20.02.2024
# @author: Aleksey Komissarov
# @contact: ad3002@gmail.com

from .sequences import get_revcomp
from collections import Counter
from tqdm import tqdm

def best_kmer_for_start(arrays, k=5):
    k = 5
    kmers = Counter()
    for array in arrays:
        repeat = "".join(array)
        prev_kmer = None
        for i in range(len(repeat)-k+1):
            kmer = repeat[i:i+k]
            if kmer == prev_kmer:
                continue
            prev_kmer = kmer
            kmers[kmer] += 1

    best_kmer = None
    max_f1 = 0
    for kmer, _ in tqdm(kmers.most_common(100)):
        copy_number = []
        for array in arrays:
            for repeat in array:
                copy_number.append(repeat.count(kmer))

        n = len(copy_number)
        r = len([x for x in copy_number if x > 0])/n
        p = len([x for x in copy_number if x == 1])/n
        f1 = 2 * (r * p) / (r + p)
        if f1 < 0.5:
            continue
        if f1 > max_f1:
            best_kmer = (r, p, f1, kmer)
            max_f1 = f1
    return max_f1, best_kmer

def rotate_arrays(arrays_, starting_kmer=None):

    if not starting_kmer:
        arrays = []
        for array in arrays_:
            feature1 = array.count("C") < array.count("G")
            feature2 = array.count("A") < array.count("T")
            if feature1 and feature2 or (not feature1 and not feature2):
                if feature1 and feature2:
                    arrays.append(array.split())
                else:
                    arrays.append(get_revcomp(array).split())
            else:
                if feature1 and not feature2:
                    arrays.append(get_revcomp(array).split())
                else:
                    arrays.append(array.split())
        max_f1, (r, p, f1, best_kmer) = best_kmer_for_start(arrays)
    else:
        best_kmer = starting_kmer
        best_rev_kmer = get_revcomp(best_kmer)
        arrays = []
        for array in arrays_:
            forward_feature = array.count(best_rev_kmer) < array.count(best_kmer)
            if forward_feature:
                arrays.append(array.split())
            else:
                arrays.append(get_revcomp(array).split())
        
    new_arrays = []
    for array in arrays:
        new_array = []
        prev_repeat = ""
        queue = array[::]
        while queue:
            repeat = prev_repeat + queue.pop(0)
            pos = repeat.find(best_kmer, 1)
            if pos == -1:
                prev_repeat = repeat
                continue
            new_array.append(repeat[:pos])
            prev_repeat = repeat[pos:]
        if prev_repeat:
            while True:
                pos = prev_repeat.find(best_kmer, 1)
                if pos == -1:
                    new_array.append(prev_repeat)
                    break
                new_array.append(prev_repeat[:pos])
                prev_repeat = prev_repeat[pos:]
            
        assert "".join(array) == "".join(new_array)
        
        new_arrays.append(new_array)

    return new_arrays

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/src/ArraySplitter/core_functions/io/fasta_reader.py</path>
<content>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# @created: 05.06.2011
# @author: Aleksey Komissarov
# @contact: ad3002@gmail.com

import gzip


def sc_iter_fasta_file(file_name, inmem=False, lower=False):
    """Iter over fasta file."""

    header = None
    seq = []
    if file_name.endswith(".gz"):
        opener = gzip.open
        decoder = lambda x: x.decode("utf8")
    else:
        opener = open
        decoder = lambda x: x

    with opener(file_name) as fh:
        if inmem:
            data = [decoder(x) for x in fh.readlines()]
            decoder = lambda x: x
        else:
            data = fh
        for line in data:
            line = decoder(line)
            if line.startswith(">"):
                if seq or header:
                    sequence = "".join(seq)
                    if lower:
                        sequence = sequence.lower()
                    yield header, sequence
                header = line.strip()[1:]
                seq = []
                continue
            seq.append(line.strip())
        if seq or header:
            sequence = "".join(seq)
            if lower:
                sequence = sequence.lower()
            yield header, sequence


def sc_iter_arrays_fasta_file(fasta_file):
    """Iter over fasta file and yield sequences."""
    for header, sequence in sc_iter_fasta_file(fasta_file):
        yield sequence

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/src/ArraySplitter/core_functions/io/satellome_reader.py</path>
<content>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# @created: 20.02.2024
# @author: Aleksey Komissarov
# @contact: ad3002@gmail.com

from .tab_file import sc_iter_tab_file
from ..models.trf_model import TRModel


def sc_iter_arrays_satellome_file(trf_file):
    """Iter over satellome file."""
    for trf_obj in sc_iter_tab_file(trf_file, TRModel):
        yield trf_obj.trf_array

def sc_iter_satellome_file(trf_file):
    """Iter over satellome file."""
    for trf_obj in sc_iter_tab_file(trf_file, TRModel):
        yield trf_obj.trf_id, trf_obj.trf_array

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/src/ArraySplitter/core_functions/io/__init__.py</path>
<content>

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/src/ArraySplitter/core_functions/io/block_file.py</path>
<content>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# @created: 05.06.2011
# @author: Aleksey Komissarov
# @contact: ad3002@gmail.com

"""
Classes:
    
- AbstractBlockFileIO(AbstractFileIO)
    
"""

from PyExp import AbstractFileIO


class AbstractBlockFileIO(AbstractFileIO):
    """Working with file with data organized in block, where each block starts with same token.

    Public methods:

    - get_block_sequence(self, head_start, next_head, fh)
    - get_blocks(self, token, fh)
    - gen_block_sequences(self, token, fh)

    Inherited public properties:

    - data  - iterable data, each item is tuple (head, body)
    - N     - a number of items in data

    Overrided public methods:

    - __init__(self, token)
    - read_from_file(self, input_file)
    - read_online(self, input_file) ~> item

    Inherited public methods:

    - read_from_db(self, db_cursor)
    - write_to_file(self, output_file)
    - write_to_db(self, db_cursor)
    - read_as_iter(self, source)
    - iterate(self) ~> item of data
    - do(self, cf, args) -> result
    - process(self, cf, args)
    - clear(self)
    - do_with_iter(self, cf, args) -> [result,]
    - process_with_iter(self, cf, args)

    """

    def __init__(self, token, **args):
        """Overrided. Set token velue."""
        super(AbstractBlockFileIO, self).__init__(**args)
        self.token = token
        self.wise_opener = self.get_opener()

    def read_from_file(self, input_file):
        """Overrided. Read data from given input_file."""
        with self.wise_opener(input_file, "r") as fh:
            for head, body, start, next in self.gen_block_sequences(self.token, fh):
                self.data.append((head, body, start, next))

    def read_online(self, input_file):
        """Overrided. Yield items from data online from input_file."""
        with self.wise_opener(input_file, "r") as fh:
            for head, body, start, next in self.gen_block_sequences(self.token, fh):
                yield (head, body, start, next)

    def get_block_sequence(self, head_start, next_head, fh):
        """Get a data block (head, seq, head_start, head_end).

        Arguments:

        - head_start -- a head starting position in a file
        - next_head  -- a next to head starting position in a file
        - fh         -- an open file handler

        Return format:

        - head       -- a block head
        - seq        -- a block body
        - head_start -- a file pointer to block start
        - head_end   -- a file pointer to next block start or 0
        """
        head_start = int(head_start)
        next_head = int(next_head)
        sequence = ""
        fh.seek(head_start)
        pos = head_start
        if next_head:
            head = fh.readline()
            pos = fh.tell()
            while pos != next_head:
                temp_seq = fh.readline()
                pos = fh.tell()
                if pos != next_head:
                    sequence += temp_seq
            sequence += temp_seq
        else:
            head = fh.readline()
            fasta_list = fh.readlines()
            sequence = "".join(fasta_list)
        return (head, sequence, head_start, next_head)

    def get_blocks(self, token, fh):
        """Get a list of the token positions in given file (first, next).
        For the last string the function returns (last string, 0).

        Arguments:

        - token -- the token indicating a block start
        - fh    -- an open file handler

        Return format: a list of (start, next start) tuples
        """
        fh.seek(0)
        header_start_list = []
        here = 0
        wrong = 0
        start = -1
        while fh:
            here = fh.tell()
            line = fh.readline()
            if len(line) == 0:
                header_start_list.append((start, 0))
                break
            if line.startswith(token):
                if start == -1:
                    start = here
                else:
                    header_start_list.append((start, here))
                    start = here
        return header_start_list

    def gen_block_sequences(self, token, fh):
        """Yield (head, seq, head_start, head_end) tuplefor given fh for open file.

        Arguments:

        - token -- the token indicating a block start
        - fh    -- an open file handler

        Return format:

        - head       -- a block head
        - seq        -- a block body
        - head_start -- a file pointer to block start
        - head_end   -- a file pointer to next block start or 0

        """
        header_start_list = self.get_blocks(token, fh)
        for x, y in header_start_list:
            # yeild sequence by coordinates
            yield self.get_block_sequence(x, y, fh)

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/src/ArraySplitter/core_functions/io/trf_reader.py</path>
<content>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# @created: 05.06.2011
# @author: Aleksey Komissarov
# @contact: ad3002@gmail.com

"""
Classes:

- TRFFileIO(AbstractBlockFileIO)

"""

from .block_file import AbstractBlockFileIO
from ..models.trf_model import TRModel


class TRFFileIO(AbstractBlockFileIO):
    """Working with raw output from TRF."""

    def __init__(self):
        """Overridden. Hardcoded start token."""
        token = "Sequence:"
        super(TRFFileIO, self).__init__(token)

    def iter_parse(self, trf_file, filter=True):
        """Iterate over raw trf data and yield TRFObjs.

        Args:
            trf_file (str): The path to the TRF file.
            filter (bool, optional): Whether to filter the TRF objects. Defaults to True.

        Yields:
            TRModel: The parsed TRF object.

        """
        for ii, (head, body, start, next) in enumerate(self.read_online(trf_file)):
            head = head.replace("\t", " ")
            obj_set = []
            n = body.count("\n")
            for i, line in enumerate(self._gen_data_line(body)):
                trf_obj = TRModel()
                trf_obj.set_raw_trf(head, None, line)
                obj_set.append(trf_obj)
            yield from obj_set

    def _gen_data_line(self, data):
        """Generate lines of data from the raw TRF data.

        Args:
            data (str): The raw TRF data.

        Yields:
            str: A line of data.

        """
        for line in data.split("\n"):
            line = line.strip()
            if line.startswith("Sequence"):
                continue
            if line.startswith("Parameters"):
                continue
            if not line:
                continue
            yield line


def sc_iter_arrays_trf_file(file_name):
    """Iter over trf file."""
    reader = TRFFileIO()
    for trf_obj in reader.iter_parse(file_name):
        yield trf_obj.trf_array

def sc_iter_trf_file(file_name):
    """Iter over trf file."""
    reader = TRFFileIO()
    for trf_obj in reader.iter_parse(file_name):
        yield trf_obj.trf_id, trf_obj.trf_array

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/src/ArraySplitter/core_functions/io/tab_file.py</path>
<content>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# @created: 05.06.2011
# @author: Aleksey Komissarov
# @contact: ad3002@gmail.com
"""
Classes:
    
- TabDelimitedFileIO(AbstractFileIO)
    
"""
import csv
import os
import tempfile

from PyExp import AbstractFileIO

csv.field_size_limit(1000000000)


class TabDelimitedFileIO(AbstractFileIO):
    """Working with tab delimited file.

    Public methods:

    - sort(self, sort_func, reversed=False)

    Inherited public properties:

    - data  - iterable data, each item is tuple (head, body)
    - N     - a number of items in data

    Overrided public methods:

    - __init__(self, skip_first=False, format_func=None, delimeter="\\t", skip_startswith=None)
    - write_to_file(self, output_file)
    - read_from_file(self, input_file)
    - read_online(self, input_file) ~> item

    Inherited public methods:

    - read_from_db(self, db_cursor)
    - write_to_db(self, db_cursor)
    - read_as_iter(self, source)
    - iterate(self) ~> item of data
    - do(self, cf, args) -> result
    - process(self, cf, args)
    - clear(self)
    - do_with_iter(self, cf, args) -> [result,]
    - process_with_iter(self, cf, args)

    """

    def __init__(
        self, skip_first=False, format_func=None, delimeter="\t", skip_startswith="#"
    ):
        """Overrided. Set token velue.

        Keyword arguments:

        - skip_first   -- skip first line in file
        - format_funcs -- list of functions that format corresponding item in tab delimited line
        - delimeter    -- line delimeter
        - skip_startswith -- skip lines starts with this value
        """
        super(TabDelimitedFileIO, self).__init__()

        self.skip_first = skip_first
        self.format_func = format_func
        self.delimeter = delimeter
        self.skip_startswith = skip_startswith

    def read_from_file(self, input_file):
        """OVerrided. Read data from tab delimeted input_file."""
        with open(input_file) as fh:
            self._data = fh.readlines()
        if self.skip_first:
            self._data.pop(0)
        if self.skip_startswith:
            self._data = [
                line for line in self._data if not line.startswith(self.skip_startswith)
            ]
        self._data = [self._process_tab_delimeited_line(line) for line in self._data]

    def read_online(self, input_file):
        """Overrided. Yield items online from data from input_file."""
        with open(input_file) as fh:
            for i, line in enumerate(fh):
                if self.skip_first and i == 0:
                    continue
                if self.skip_startswith and line.startswith(self.skip_startswith):
                    continue
                yield self._process_tab_delimeited_line(line)

    def _process_tab_delimeited_line(self, line):
        """Format line with format_func."""

        line = line.strip().split(self.delimeter)
        if self.format_func:
            assert hasattr(self.format_func, "__call__")
            line = self.format_func(line)
        return line

    def _all_str(self, line):
        """Convert to string all items in line."""
        return [str(x) for x in line]

    def write_to_file(self, output_file):
        """Overrided. Write data to tab delimited output_file."""
        self._data = ["\t".join(self._all_str(line)) for line in self._data]
        with open(output_file, "w") as fh:
            fh.writelines(self._data)


def sc_iter_tab_file(
    input_file,
    data_type,
    skip_starts_with="#",
    remove_starts_with=None,
    preprocess_function=None,
    check_function=None,
):
    """Iter over tab file, yield an object of given data_type."""

    temp_file = tempfile.NamedTemporaryFile(delete=False)
    temp_file_name = temp_file.name

    if remove_starts_with:
        with open(input_file, "r") as fh:
            data = fh.readlines()
        data = [x for x in data if not x.startswith(remove_starts_with)]
        with open(temp_file_name, "w") as fh:
            fh.writelines(data)
        input_file = temp_file_name
    if preprocess_function:
        with open(input_file, "r") as fh:
            data = fh.readlines()
        data = [preprocess_function(x) for x in data]
        with open(temp_file_name, "w") as fh:
            fh.writelines(data)
        input_file = temp_file_name
    if check_function:
        with open(input_file, "r") as fh:
            data = fh.readlines()
        data = [x for x in data if check_function(x)]
        with open(temp_file_name, "w") as fh:
            fh.writelines(data)
        input_file = temp_file_name
    with open(input_file) as fh:
        fields = data_type().dumpable_attributes
        for data in csv.DictReader(
            fh, fieldnames=fields, delimiter="\t", quoting=csv.QUOTE_NONE
        ):
            if skip_starts_with:
                if data[fields[0]].startswith(skip_starts_with):
                    continue
            obj = data_type()
            obj.set_with_dict(data)
            yield obj
    if os.path.isfile(temp_file_name):
        os.unlink(temp_file_name)


def sc_iter_simple_tab_file(input_file):
    """Iter tab file, yield a list."""
    with open(input_file) as fh:
        for data in csv.reader(fh, delimiter="\t", quoting=csv.QUOTE_NONE):
            yield data


def sc_read_dictionary(dict_file, value_func=None):
    """Read file of tab-dilimited pairs.
    key\tvalue
    """
    result = {}
    with open(dict_file) as fh:
        for data in csv.reader(fh, delimiter="\t", quoting=csv.QUOTE_NONE):
            if hasattr(value_func, "__call__"):
                data[1] = value_func(data[1])
            result[data[0]] = data[1]
    return result


def sc_write_model_to_tab_file(output_file, objs):
    """Write model obj to tab-delimited file."""
    with open(output_file, "w") as fh:
        for obj in objs:
            fh.write(str(obj))


def sc_read_simple_tab_file(input_file, skip_first=False):
    """Iter tab file, yield a list."""
    result = []
    with open(input_file) as fh:
        if skip_first:
            fh.readline()
        for data in csv.reader(fh, delimiter="\t", quoting=csv.QUOTE_NONE):
            result.append(data)
    return result

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/src/ArraySplitter/core_functions/models/__init__.py</path>
<content>

</content>
</file>

<file>
<path>/Users/akomissarov/Dropbox2/Dropbox/workspace/ArraySplitter/src/ArraySplitter/core_functions/models/trf_model.py</path>
<content>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# @created: 14.06.2011
# @author: Aleksey Komissarov
# @contact: ad3002@gmail.com

import re

from PyExp import AbstractModel

from ..tools.parsers import (parse_chromosome_name,
                                                        parse_fasta_head,
                                                        trf_parse_head,
                                                        trf_parse_line)


def get_gc(sequence):
    """Count GC content."""
    length = len(sequence)
    if not length:
        return 0
    count_c = sequence.count("c") + sequence.count("C")
    count_g = sequence.count("g") + sequence.count("G")
    gc = float(count_c + count_g) / float(length)
    return float(gc)


def clear_sequence(sequence):
    """Clear sequence (full alphabet):

    - lower case
    - \s -> ""
    - [^actgn] -> ""
    """
    sequence = sequence.strip().upper()
    sequence = re.sub("\s+", "", sequence)
    return re.sub("[^actgnuwsmkrybdhvACTGNUWSMKRYBDHV\-]", "", sequence)


class TRModel(AbstractModel):
    """Class for tandem repeat wrapping

    Public properties:

    Indexes:

    - id,
    - giid,
    - trf_id,
    - project,
    - trf_gi

    Coordinates:

    - trf_l_ind,
    - trf_r_indel

    TR:

    - trf_period,
    - trf_n_copy,
    - trf_pmatch,
    - trf_pvar,
    - trf_entropy,
    - trf_array_length,
    - trf_joined,
    - trf_chr

    Sequence:

    - trf_consensus,
    - trf_array

    GC%:

    - trf_array_gc,
    - trf_consensus_gc

    Other:

    - trf_head,
    - trf_params

    Annotation:

    - trf_family,
    - trf_subfamily,
    - trf_subsubfamily,
    - trf_hor,
    - trf_n_chrun,
    - trf_n_refgenome,
    - trf_ref_annotation,
    - trf_bands_refgenome,
    - trf_repbase,
    - trf_strand

    Dumpable attributes:

    - "project",
    - "id" (int),
    - "trf_id" (int),
    - "trf_type",
    - "trf_family",
    - "trf_family_prob",
    - "trf_l_ind" (int),
    - "trf_r_ind" (int),
    - "trf_period" (int),
    - "trf_n_copy" (float),
    - "trf_pmatch" (float),
    - "trf_pvar" (float),
    - "trf_consensus",
    - "trf_array",
    - "trf_array_gc" (float),
    - "trf_consensus_gc" (float),
    - "trf_gi",
    - "trf_head",
    - "trf_param",
    - "trf_array_length" (int),
    - "trf_chr",
    - "trf_joined" (int),
    - "trf_superfamily",
    - "trf_superfamily_ref",
    - "trf_superfamily_self",
    - "trf_subfamily",
    - "trf_subsubfamily",
    - "trf_family_network",
    - "trf_family_self",
    - "trf_family_ref",
    - "trf_hor" (int),
    - "trf_n_chrun" (int),
    - "trf_ref_annotation",
    - "trf_bands_refgenome",
    - "trf_repbase",
    - "trf_strand",

    """

    dumpable_attributes = [
        "project",
        "id",
        "trf_id",
        "trf_type",
        "trf_family",
        "trf_family_prob",
        "trf_l_ind",
        "trf_r_ind",
        "trf_period",
        "trf_n_copy",
        "trf_pmatch",
        "trf_pvar",
        "trf_entropy",
        "trf_consensus",
        "trf_array",
        "trf_array_gc",
        "trf_consensus_gc",
        "trf_gi",
        "trf_head",
        "trf_param",
        "trf_array_length",
        "trf_chr",
        "trf_joined",
        "trf_superfamily",
        "trf_superfamily_ref",
        "trf_superfamily_self",
        "trf_subfamily",
        "trf_subsubfamily",
        "trf_family_network",
        "trf_family_self",
        "trf_family_ref",
        "trf_hor",
        "trf_n_chrun",
        "trf_ref_annotation",
        "trf_bands_refgenome",
        "trf_repbase",
        "trf_strand",
    ]

    int_attributes = [
        "trf_l_ind",
        "trf_r_ind",
        "trf_period",
        "trf_array_length",
        "trf_joined",
        "trf_hor",
        "trf_n_chrun",
    ]

    float_attributes = [
        "trf_n_copy",
        "trf_family_prob",
        "trf_entropy",
        "trf_pmatch",
        "trf_pvar",
        "trf_array_gc",
        "trf_consensus_gc",
    ]

    def set_project_data(self, project):
        """Add project data to self.project."""
        self.project = project

    def set_raw_trf(self, head, body, line):
        """Init object with data from parsed trf ouput.

        Parameters:

        - trf_obj: TRFObj instance
        - head: parsed trf head
        - body: parsed trf body
        - line: parsed trf line
        """
        self.trf_param = 0
        self.trf_head = trf_parse_head(head).strip()
        self.trf_gi = parse_fasta_head(self.trf_head)[0]
        self.trf_chr = parse_chromosome_name(self.trf_head)

        (
            self.trf_l_ind,
            self.trf_r_ind,
            self.trf_period,
            self.trf_n_copy,
            self.trf_l_cons,
            self.trf_pmatch,
            self.trf_indels,
            self.trf_score,
            self.trf_n_a,
            self.trf_n_c,
            self.trf_n_g,
            self.trf_n_t,
            self.trf_entropy,
            self.trf_consensus,
            self.trf_array,
        ) = trf_parse_line(line)

        self.trf_pmatch = float(self.trf_pmatch)
        self.trf_pvar = int(100 - float(self.trf_pmatch))

        try:
            self.trf_l_ind = int(self.trf_l_ind)
        except:
            print("Error:", self)

        self.trf_r_ind = int(self.trf_r_ind)
        self.trf_period = int(self.trf_period)
        self.trf_n_copy = float(self.trf_n_copy)

        self.trf_consensus = clear_sequence(self.trf_consensus)
        self.trf_array = clear_sequence(self.trf_array)

        self.trf_array_gc = get_gc(self.trf_array)
        self.trf_consensus_gc = get_gc(self.trf_consensus)
        self.trf_chr = parse_chromosome_name(self.trf_head)
        self.trf_array_length = len(self.trf_array)

    def get_header_string(self):
        """Get header string for tsv file."""
        data = "\t".join(self.dumpable_attributes)
        return f"#{data}\n"

    def get_fasta_repr(self, add_project=False):
        """Get array fasta representation, head - trf_id."""
        if add_project:
            return ">%s_%s\n%s\n" % (self.trf_id, self.project, self.trf_array)
        else:
            return ">%s\n%s\n" % (self.trf_id, self.trf_array)

    @property
    def fasta(self):
        return self.get_fasta_repr()

    def get_gff3_string(
        self,
        chromosome=True,
        trs_type="complex_tandem_repeat",
        probability=1000,
        tool="PySatDNA",
        prefix=None,
        properties=None,
        force_header=False,
    ):
        """Return TR in gff format."""
        if chromosome and self.trf_chr and self.trf_chr != "?":
            seqid = self.trf_chr
        elif self.trf_gi and self.trf_gi != "Unknown":
            seqid = self.trf_gi
        else:
            seqid = self.trf_head
        features = []
        if not properties:
            properties = {}
        for name, attr in properties.items():
            features.append("%s=%s" % (name, getattr(self, attr)))
        features = ";".join(features)
        if prefix:
            seqid = prefix + seqid
        if self.trf_l_ind < self.trf_r_ind:
            strand = "+"
        else:
            strand = "-"
            self.trf_l_ind, self.trf_r_ind = self.trf_r_ind, self.trf_l_ind

        if force_header:
            seqid = self.trf_head
        d = (
            seqid,
            tool,
            trs_type,
            self.trf_l_ind,
            self.trf_r_ind,
            probability,
            strand,
            ".",
            features,
        )
        return "%s\n" % "\t".join(map(str, d))

    def get_bed_string(self):
        """Return TR in bed format."""
        if self.trf_l_ind < self.trf_r_ind:
            strand = "+"
        else:
            strand = "-"
            self.trf_l_ind, self.trf_r_ind = self.trf_r_ind, self.trf_l_ind

        seqid = self.trf_head
        d = (
            seqid,
            self.trf_l_ind,
            self.trf_r_ind,
        )
        return "%s\n" % "\t".join(map(str, d))

</content>
</file>

